{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom glob import glob\nfrom pathlib import Path\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfilenames = glob(f'/kaggle/working/*.*')\nfor filename in filenames:\n    print(filename)\n    if 'scores.logs' in filename:\n        continue\n#     Path(filename).unlink()\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-30T15:18:35.802684Z","iopub.execute_input":"2023-05-30T15:18:35.803147Z","iopub.status.idle":"2023-05-30T15:18:35.829457Z","shell.execute_reply.started":"2023-05-30T15:18:35.803110Z","shell.execute_reply":"2023-05-30T15:18:35.828454Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/ozon-task-five/preprocess_test_pairs_wo_target.pkl\n/kaggle/input/ozon-task-five/preprocess_train_pairs.pkl\n/kaggle/input/ozon-task-five/dts_object.pkl\n/kaggle/input/ozon-task-five/test_pairs_wo_target.pkl\n/kaggle/input/ozon-task-five/scores.logs\n/kaggle/input/ozon-task-five/train_pairs.pkl\n/kaggle/input/ozon-task-five/preprocess_data.pkl\n/kaggle/working/__notebook_source__.ipynb\n","output_type":"stream"}]},{"cell_type":"code","source":"PARQUET_ENGINE = 'pyarrow'\nDATASET_PATH = Path('/kaggle/input/ozon-task-five')\nWORK_PATH = Path('.')\nPREDICTIONS_DIR = Path('.')","metadata":{"execution":{"iopub.status.busy":"2023-05-30T15:18:35.834185Z","iopub.execute_input":"2023-05-30T15:18:35.836253Z","iopub.status.idle":"2023-05-30T15:18:35.844547Z","shell.execute_reply.started":"2023-05-30T15:18:35.836218Z","shell.execute_reply":"2023-05-30T15:18:35.843600Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nfrom pathlib import Path\nfrom datetime import date, datetime, timedelta\nfrom tqdm import tqdm\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport optuna\nfrom optuna.integration import CatBoostPruningCallback\n\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\nfrom sklearn.metrics import precision_recall_curve, auc\n\nfrom time import time\n\n__import__(\"warnings\").filterwarnings('ignore')\n\n\ndef convert_seconds(time_apply):\n    # print(type(time_apply), time_apply)\n    try:\n        time_apply = float(time_apply)\n    except ValueError:\n        time_apply = 0\n    if isinstance(time_apply, (int, float)):\n        hrs = time_apply // 3600\n        mns = time_apply % 3600\n        sec = mns % 60\n        time_string = ''\n        if hrs:\n            time_string = f'{hrs:.0f} час '\n        if mns // 60 or hrs:\n            time_string += f'{mns // 60:.0f} мин '\n        return f'{time_string}{sec:.1f} сек'\n\n\ndef print_time(time_start):\n    \"\"\"\n    Печать времени выполнения процесса\n    :param time_start: время запуска в формате time.time()\n    :return:\n    \"\"\"\n    time_apply = time() - time_start\n    print(f'Время обработки: {convert_seconds(time_apply)}')\n\n\ndef print_msg(msg):\n    print(msg)\n    return time()\n\n\ndef memory_compression(df, use_category=True, use_float=True):\n    \"\"\"\n    Изменение типов данных для экономии памяти\n    :param df: исходный ДФ\n    :param use_category: преобразовывать строки в категорию\n    :param use_float: преобразовывать float в пониженную размерность\n    :return: сжатый ДФ\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        # print(f'{col} тип: {df[col].dtype}', str(df[col].dtype)[:4])\n\n        if str(df[col].dtype)[:4] in 'datetime':\n            continue\n\n        elif str(df[col].dtype) not in ('object', 'category'):\n            col_min = df[col].min()\n            col_max = df[col].max()\n            if str(df[col].dtype)[:3] == 'int':\n                if col_min > np.iinfo(np.int8).min and \\\n                        col_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif col_min > np.iinfo(np.int16).min and \\\n                        col_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif col_min > np.iinfo(np.int32).min and \\\n                        col_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif col_min > np.iinfo(np.int64).min and \\\n                        col_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            elif use_float and str(df[col].dtype)[:5] == 'float':\n                if col_min > np.finfo(np.float16).min and \\\n                        col_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif col_min > np.finfo(np.float32).min and \\\n                        col_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n        elif use_category and str(df[col].dtype) == 'object':\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    print(f'Исходный размер датасета в памяти '\n          f'равен {round(start_mem, 2)} мб.')\n    print(f'Конечный размер датасета в памяти '\n          f'равен {round(end_mem, 2)} мб.')\n    print(f'Экономия памяти = {(1 - end_mem / start_mem):.1%}')\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-05-30T15:18:35.855387Z","iopub.execute_input":"2023-05-30T15:18:35.856206Z","iopub.status.idle":"2023-05-30T15:18:38.081788Z","shell.execute_reply.started":"2023-05-30T15:18:35.856174Z","shell.execute_reply":"2023-05-30T15:18:38.080815Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install pymorphy2\n!pip install pytils\n!pip install transliterate ","metadata":{"execution":{"iopub.status.busy":"2023-05-30T15:18:38.086918Z","iopub.execute_input":"2023-05-30T15:18:38.089346Z","iopub.status.idle":"2023-05-30T15:19:27.538383Z","shell.execute_reply.started":"2023-05-30T15:18:38.089313Z","shell.execute_reply":"2023-05-30T15:19:27.537249Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting pymorphy2\n  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\nCollecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.10/site-packages (from pymorphy2) (0.6.2)\nInstalling collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\nSuccessfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pytils\n  Downloading pytils-0.4.1.tar.gz (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: pytils\n  Building wheel for pytils (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytils: filename=pytils-0.4.1-py3-none-any.whl size=32538 sha256=1bfce9bf48cf2ac79d516e8cb4c5a45bec9ad024e92218fd86b5ec408abd1fd2\n  Stored in directory: /root/.cache/pip/wheels/5a/eb/7c/3b6f0c25815749883152b2caca34c35dbaab13ec2864270cbd\nSuccessfully built pytils\nInstalling collected packages: pytils\nSuccessfully installed pytils-0.4.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting transliterate\n  Downloading transliterate-1.10.2-py2.py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from transliterate) (1.16.0)\nInstalling collected packages: transliterate\nSuccessfully installed transliterate-1.10.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from ast import literal_eval\nfrom tqdm import tqdm\n\nfrom sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n\nimport spacy\nimport pymorphy2\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\nfrom pytils.translit import translify, detranslify\nfrom transliterate import translit\nfrom fuzzywuzzy import fuzz\n\nfrom fasttext.FastText import _FastText","metadata":{"execution":{"iopub.status.busy":"2023-05-30T15:19:27.541528Z","iopub.execute_input":"2023-05-30T15:19:27.541941Z","iopub.status.idle":"2023-05-30T15:19:43.030146Z","shell.execute_reply.started":"2023-05-30T15:19:27.541898Z","shell.execute_reply":"2023-05-30T15:19:43.029136Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class TextLemmatization:\n    def __init__(self):\n        model_file = DATASET_PATH.joinpath('lid.176.bin')\n        if not model_file.is_file():\n            model_file = r'D:\\python-txt\\my_addons\\lid.176.ftz'\n        self.ft_model = _FastText(model_path=str(model_file))\n        self.snowball_ru = SnowballStemmer(language='russian')\n        self.snowball_en = SnowballStemmer(language='english')\n        self.morphatizer = pymorphy2.MorphAnalyzer()\n        self.lemmatizer = WordNetLemmatizer()\n        self.ru_en_stop = set(stopwords.words('russian') + stopwords.words('english'))\n        self.pattern_date = r'[0-9_\\.\\/\\\\-]+'\n        self.pattern_punct = r'[!@\"“’«»#$%&\\'()*+,—/:;<=>?^_`{|}~\\[\\]]'\n        self.digits = ''.join(str(i) for i in range(10))\n        self.nlp_ru = spacy.load(\"ru_core_news_sm\")\n        self.nlp_en = spacy.load(\"en_core_web_sm\")\n        self.text_language = 'russian'\n        self.use_unique_words = True\n        self.singular = ('bes', 'bez', 'bolee', 'budet', 'da', 'do', 'dva', 'ee', 'ego',\n                         'emu', 'esli', 'gde', 'i', 'ih', 'ili', 'iz', 'k', 'kak', 'ko',\n                         'kogda', 'kto', 'li', 'mne', 'mnogo', 'na', 'nad', 'nado', 'nas',\n                         'ne', 'nego', 'ni', 'nim', 'nu', 'o', 'ob', 'oba', 'odin', 'ona',\n                         'oni', 'ot', 'pered', 'po', 'pod', 'potom', 'potomu', 'pri', 'pro',\n                         'raz', 's', 'sebe', 'tak', 'tam', 'tem', 'togda', 'tri', 'tut',\n                         'u', 'v', 'vam', 'vas', 'vdrug', 'vid', 'vo', 'vot', 'vse',\n                         'vsegda', 'vsego', 'za', 'zatem')\n\n    def identification_language(self, text):\n        \"\"\"\n        Определение языка текста и запись его в переменную self.text_language\n        :param text: текст\n        :return: код языка\n        \"\"\"\n        predict_lang = self.ft_model.predict(text)\n        print('predict_lang', predict_lang)\n        detect_language = {'en': 'english'}.get(predict_lang[0][0][-2:], 'russian')\n\n        # тут определение, что это транслитерация\n        if self.detect_translit(text):\n            detect_language = 'transliteration'\n\n        self.text_language = detect_language\n        print('Определен язык:', detect_language)\n        return detect_language\n\n    @staticmethod\n    def del_empty_lines(text):\n        \"\"\"\n        Удаление пустых строк\n        :param text: текст\n        :return: очищенный текст\n        \"\"\"\n        text = re.sub('\\ufeff', '', str(text))\n        # разделение на строки\n        text_list = re.split(r'\\n', text)\n        # склейка обратно в строку, только без пустых строк\n        text = '\\n'.join(s.strip() for s in text_list if s.strip())\n        return text.strip()\n\n    def preprocess(self, tokens):\n        \"\"\"\n        В функции отфильтруем также числа, проверив первый символ.\n        Так, это поможет избавиться от «2019», но не от Covid-19.\n        :param tokens: список слов\n        :return: список нормализованных слов\n        \"\"\"\n        return [self.morphatizer.parse(word)[0].normal_form for word in tokens if\n                (word[0] not in self.digits and word not in self.ru_en_stop)]\n\n    def normalize_text(self, input_text, language=None, unique_words=None):\n        \"\"\"\n        Перевод текста в нижний регистр, удаление дат, удаление знаков пунктуации\n        деление текста на слова\n        :param input_text: текст\n        :param language: язык текста\n        :param unique_words: оставляем только уникальные слова\n        :return: список слов\n        \"\"\"\n        if language is None:\n            language = self.text_language\n        if unique_words is None:\n            unique_words = self.use_unique_words\n\n        # убираем из текста цифры и переводим в нижний регистр\n        # input_text = re.sub(self.pattern_date, ' ', str(input_text).lower())\n\n        # убираем из текста знаки пунктуации\n        input_text = re.sub(self.pattern_punct, ' ', input_text.lower())\n\n        # если язык = 'transliteration'\n        if language == 'transliteration':\n            language = 'russian'\n            self.text_language = language\n            trans = input_text\n            # вариант детранслитерации № 1\n            input_text = detranslify(trans)\n            # print('Вариант 1:', input_text)\n            # # вариант детранслитерации № 2\n            # input_text = translit(trans, 'ru')\n            # print('Вариант 2:', input_text)\n\n        words = word_tokenize(input_text, language=language)\n        out_words = []\n        for word in words:\n            # пропускаем буквенные слова из одной буквы, цифры оставляем\n            if len(word) == 1 and word.isalpha():\n                continue\n            # если собираем только уникальные слова\n            if unique_words and word in out_words:\n                continue\n            if word not in self.ru_en_stop:\n                out_words.append(word)\n\n        return out_words\n\n    def stem_text(self, list_words, language=None, unique_words=None):\n        if language is None:\n            language = self.text_language\n        if unique_words is None:\n            unique_words = self.use_unique_words\n\n        snowball = self.snowball_ru if language == 'russian' else self.snowball_en\n        stem_words = []\n        for word in list_words:\n            stem_word = snowball.stem(word)\n            if stem_word not in self.ru_en_stop and (\n                    not unique_words or (unique_words and stem_word not in stem_words)):\n                stem_words.append(stem_word)\n        return stem_words\n\n    def lemm_text(self, list_words, language=None, unique_words=None):\n        \"\"\"\n        Лемматизация текста\n        :param list_words: список слов\n        :param language: язык\n        :param unique_words: возвращать только уникальные слова\n        :return: список словарных лемм, список топонимов, список имен персон\n        \"\"\"\n        if language is None:\n            language = self.text_language\n        if unique_words is None:\n            unique_words = self.use_unique_words\n\n        lemm_words = []\n\n        for word in list_words:\n            parsed = self.morphatizer.parse(word)[0]\n            # print(parsed.normal_form, parsed.tag)\n            if language == 'russian':\n                lemma = parsed.normal_form\n                if lemma not in self.ru_en_stop and (\n                        not unique_words or (unique_words and lemma not in lemm_words)):\n                    lemm_words.append(lemma)\n            else:\n                # получим токен из словаря, если его нет (такое возможно для составных слов)\n                # --> возьмем первой токен из токенизированного одного слова\n                lemma = self.lemmatizer.lemmatize(word).lower()\n                if lemma not in self.ru_en_stop and (\n                        not unique_words or (unique_words and lemma not in lemm_words)):\n                    lemm_words.append(lemma)\n\n        return lemm_words\n\n    def make_sentences(self, text):\n        \"\"\"\n        Разделение текста на предложения\n        :param text: текст\n        :return: список предложений\n        \"\"\"\n        return sent_tokenize(self.del_empty_lines(text))\n\n    def detect_translit(self, text):\n        \"\"\"\n        Определение является ли текст транслитерацией\n        :param text: текст\n        :return: True|False - транлитерация или нет\n        \"\"\"\n        # убираем из текста знаки пунктуации\n        _text = re.sub(self.pattern_punct, ' ', text.lower())\n        # делим на слова и формируем список коротких слов\n        small_words = [*filter(lambda x: len(x) < 4 and x.isalpha(), word_tokenize(_text))]\n        # выделяем из текста однозначно транслитерированные короткие русские слова\n        trans_words = re.findall(fr\"\\W({'|'.join(self.singular)})\\W\", text, flags=re.I)\n        # количество trans_words составляет больше половины small_words - это транслитерация\n        # print(len(trans_words), len(small_words))\n        return len(trans_words) >= len(small_words) * 0.5\n\n\nclass DataPreprocess:\n    def __init__(self):\n        self.staples = {'{': '}', '[': ']'}\n        self.categories = None\n        self.columns = {'color_parsed': 'color',\n                        'pic_embeddings_resnet_v1': 'pic_emb',\n                        'main_pic_embeddings_resnet_v1': 'main_pic',\n                        'name_bert_64': 'name_bert',\n                        'characteristic_attributes_mapping': 'ch_att_map'}\n\n    @staticmethod\n    def make_sample(df, pairs, rows=500):\n        \"\"\"\n        Для опытов оставим небольшой сэмпл из данных и виде первых строк\n        :param df: ДФ\n        :param rows: количество строк ДФ\n        :return: ДФ сэмпла данных\n        \"\"\"\n        pair = pairs.iloc[:rows]\n        idxs = set(pair.variantid1.to_list() + pair.variantid2.to_list())\n        if 'variantid' in df.columns:\n            df = df[df.variantid.isin(idxs)]\n        return df, pair\n\n    def item_from_text(self, cell_text):\n        value = np.NaN\n        if not pd.isna(cell_text):\n            cell_text = cell_text.strip()\n            ch_fst = cell_text[0]\n            ch_lst = cell_text[-1]\n            # открывающая и закрывающая скобки должны быть и быть одинаковыми\n            if ch_fst in self.staples.keys() and ch_lst == self.staples[ch_fst]:\n                value = literal_eval(cell_text)\n        return value\n\n    @staticmethod\n    def str_from_dict_values(items):\n        \"\"\"\n        Пробразование значений словаря в строку слов, разделенных пробелами\n        :param items: словарь\n        :return: строка из отсортированных слов\n        \"\"\"\n        flatten_str = []\n        [flatten_str.extend(item) for item in items.values()]\n        return ' '.join(s for s in map(str.lower, map(str.strip, flatten_str))\n                        if not s.startswith('http'))\n\n    def fit(self, df):\n        \"\"\"\n        Формирование фич\n        :param df: исходный ФД\n        :return: обработанный ДФ\n        \"\"\"\n        self.categories = {cat: f'cat_{idx}' for idx, cat in\n                           enumerate(sorted(set(df['categories'].unique())), 1)}\n        return df\n\n    @staticmethod\n    def mean_pic_embeddings(pic_embeddings):\n        \"\"\"\n        Формирование усредненного ембендинга дополнительных картинок\n        :param pic_embeddings: массив с ембендингами\n        :return: усредненный ембендинг\n        \"\"\"\n        if pic_embeddings.shape[0] > 1 and len(pic_embeddings.shape) > 1:\n            return np.mean(pic_embeddings, axis=0)\n        elif pic_embeddings.shape[0] == 128:\n            return pic_embeddings\n        return pic_embeddings[0]\n\n    def transform(self, df, model_columns=None):\n        \"\"\"\n        Формирование остальных фич\n        :param df: ДФ\n        :param model_columns: список колонок, которые будут использованы в модели\n        :return: ДФ с фичами\n        \"\"\"\n        tqdm.pandas()\n\n        df['characteristic_attributes_mapping'].fillna(\"{'0': ['None']}\", inplace=True)\n\n        # преобразование строки в словарь\n        for col, new_col, func in (('characteristic_attributes_mapping', 'cam',\n                                    lambda x: self.item_from_text(x.lower())),\n                                   ('categories', 'cat', self.item_from_text)):\n            print(f'Преобразую колонку \"{col}\"')\n            df[new_col] = df[col].progress_apply(func)\n\n        # преобразование списка в numpy.ndarray\n        df['name_bert_64'] = df['name_bert_64'].map(np.array)\n\n        # заполним пропуски в дополнительных картинках главной картинкой\n        df['pic_embeddings_resnet_v1'] = df['pic_embeddings_resnet_v1'].fillna(\n            df['main_pic_embeddings_resnet_v1'])\n\n        # из вложенного numpy.ndarray достаем нулевой элемент если это массив\n        df['main_pic_embeddings_resnet_v1'] = df['main_pic_embeddings_resnet_v1'].map(\n            lambda x: x[0] if isinstance(x, np.ndarray) else np.zeros(128))\n\n        print('Объединяю ембендинги для наименования и картинки')\n        df['name_pic'] = df.progress_apply(\n            lambda row: np.concatenate((row.name_bert_64,\n                                        row.main_pic_embeddings_resnet_v1), axis=0),\n            axis=1)\n\n        # print('Усредняю ембендинги для дополнительных картинок')\n        # df['pic_emb_mean'] = df.pic_embeddings_resnet_v1.progress_apply(\n        #     lambda x: self.mean_pic_embeddings(x))\n\n        df['categories'] = df['categories'].map(self.categories)\n        df['color_parsed'] = df['color_parsed'].str.join(',')\n\n        # добавление количества товара отдельно для 3-й и 4-й категорий\n        df['cat3'] = df['cat'].apply(lambda x: x['3'])\n        df['cat3_cnt'] = df.groupby('cat3')['name'].transform('count')\n        df['cat4_cnt'] = df.groupby('categories')['name'].transform('count')\n\n        # переименуем колонки в более удобный вид\n        if self.columns:\n            df.rename(columns=self.columns, inplace=True)\n\n        stm = TextLemmatization()\n\n        df['cam'].fillna({0: ['none']}, inplace=True)\n\n        print('\\nНормализация текста в колонке \"name\"')\n        df['name_norm'] = df['name'].progress_apply(stm.normalize_text)\n        print('\\nСтемминг нормализованного текста \"name\"')\n        df['name_stem'] = df['name_norm'].progress_apply(stm.stem_text)\n        print('\\nЛемматизация нормализованного текста \"name\"')\n        df['name_lemm'] = df['name_norm'].progress_apply(stm.lemm_text)\n        df['name_st_len'] = df['name_stem'].map(len)\n        df['name_lm_len'] = df['name_lemm'].map(len)\n        df['name_stem'] = df['name_stem'].map(lambda x: ' '.join(sorted(x)))\n        df['name_lemm'] = df['name_lemm'].map(lambda x: ' '.join(sorted(x)))\n\n        print('\\nНормализация текста в колонке \"characteristic_attributes_mapping\"')\n        df['cam_len'] = df['cam'].map(len)\n        df['cam_norm'] = df['cam'].progress_apply(\n            lambda x: stm.normalize_text(self.str_from_dict_values(x)))\n        print('\\nСтемминг нормализованного текста \"characteristic_attributes_mapping\"')\n        df['cam_stem'] = df['cam_norm'].progress_apply(stm.stem_text)\n        # print('\\nЛемматизация нормализованного текста \"characteristic_attributes_mapping\"')\n        # df['cam_lemm'] = df['cam_norm'].progress_apply(stm.lemm_text)\n        df['cam_st_len'] = df['cam_stem'].map(len)\n        # df['cam_lm_len'] = df['cam_lemm'].map(len)\n        df['cam_stem'] = df['cam_stem'].map(lambda x: ' '.join(sorted(x)))\n        # df['cam_lemm'] = df['cam_lemm'].map(lambda x: ' '.join(sorted(x)))\n\n        # установим ID товара в качестве индекса\n        df.set_index('variantid', inplace=True)\n\n        print('Считаю частотную зависимость атрибутов')\n        # подсчет частотности использования характеристик\n        threshold = 0.01  # порог, ниже которого характеристика не важна\n        max_features = 99  # максимальное количество атрибутов\n        attr_keys = df['cam'].map(lambda x: set(x.keys())).to_list()\n        keys_set = set([key for keys in attr_keys for key in keys])\n        keys_dict, len_tov = {}, len(df['cam'])\n        for key in tqdm(keys_set):\n            keys_dict[key] = sum(df['cam'].apply(lambda x: len(x.get(key, '')))) / len_tov\n\n        print('Всего уникальных атрибутов =', len(keys_dict))\n\n        keys_df = pd.DataFrame(keys_dict.items(), columns=['cam_key', 'ratio'])\n        keys_df.sort_values('ratio', ascending=False, inplace=True, ignore_index=True)\n        keys_df.to_excel('cam_key.xlsx', index=False)\n\n        keys_df['key'] = keys_df.cam_key.map(lambda x: x.split(',')[0])\n        grp = keys_df.groupby('key', as_index=False).agg(\n            key_unique=('cam_key', lambda x: x.unique()),\n            ratio=('ratio', 'sum')\n        )\n        grp.sort_values('ratio', ascending=False, inplace=True, ignore_index=True)\n        grp = grp[grp.ratio >= threshold]\n        grp = grp.iloc[:max_features]\n        grp.to_excel('grp_key.xlsx', index=False)\n\n        stm = TextLemmatization()\n\n        print('Нормализация атрибутов')\n\n        from_list = lambda x: ' '.join(map(str.strip, x)).lower()\n\n        # добавление колонок со значениями атрибутов к ДФ с товаром\n        for idx, row in tqdm(grp.iterrows()):\n            attribute_name = f'atr_{idx + 1:02}'\n            # print(attribute_name, row['key'], row['key_unique'], row['key_unique'].shape)\n            for idx_key, key in enumerate(row['key_unique']):\n                if not idx_key:\n                    df[attribute_name] = df['cam'].map(\n                        lambda x: from_list(x.get(key, [''])))\n                    df[attribute_name] = df[attribute_name].map(\n                        lambda x: x if x else np.NaN)\n                else:\n                    df['temp'] = df['cam'].map(lambda x: from_list(x.get(key, [''])))\n                    df['temp'] = df['temp'].map(lambda x: x if x else np.NaN)\n                    df[attribute_name].fillna(df['temp'], inplace=True)\n\n                if key in ('гарантийный срок',):\n                    df[attribute_name] = df[attribute_name].str.findall(r'\\d+')\n                    df[attribute_name] = df[attribute_name].map(\n                        lambda x: x[0] if isinstance(x, list) and len(x) else np.NaN)\n                else:\n                    # эта операция на локальном ПК занимает 52 минуты\n                    df[attribute_name].fillna('none', inplace=True)\n                    df[attribute_name] = df[attribute_name].map(stm.normalize_text)\n                    if idx < 20:\n                        df[attribute_name] = df[attribute_name].map(\n                            lambda x: ' '.join(stm.stem_text(x)))\n                    else:\n                        df[attribute_name] = df[attribute_name].str.join(' ')\n\n            df[attribute_name].fillna('none', inplace=True)\n\n        df.drop('temp', axis=1, inplace=True)\n\n        return df\n\n    def fit_transform(self, df, model_columns=None):\n        \"\"\"\n        fit + transform\n        :param df: исходный ФД\n        :param model_columns: список колонок, которые будут использованы в модели\n        :return: ДФ с фичами\n        \"\"\"\n        df = self.fit(df)\n        df = self.transform(df, model_columns=model_columns)\n\n        return df\n\n\nclass DataTransform:\n    def __init__(self, use_catboost=True, numeric_columns=None, category_columns=None,\n                 drop_first=False, scaler=None, args_scaler=None):\n        \"\"\"\n        Преобразование данных\n        :param use_catboost: данные готовятся для catboost\n        :param numeric_columns: цифровые колонки\n        :param category_columns: категориальные колонки\n        :param drop_first: из dummy переменных удалить первую колонку\n        :param scaler: какой скайлер будем использовать\n        :param args_scaler: аргументы для скайлера, например: степень для полином.преобраз.\n        \"\"\"\n        self.use_catboost = use_catboost\n        self.category_columns = [] if category_columns is None else category_columns\n        self.numeric_columns = [] if numeric_columns is None else numeric_columns\n        self.drop_first = drop_first\n        self.exclude_columns = []\n        self.new_columns = []\n        self.comment = {'drop_first': drop_first}\n        self.transform_columns = None\n        self.scaler = scaler\n        self.args_scaler = args_scaler\n        self.preprocess_path_file = None\n        self.pair_columns = None\n\n    def cat_dummies(self, df):\n        \"\"\"\n        Отметка категориальных колонок --> str для catboost\n        OneHotEncoder для остальных\n        :param df: ДФ\n        :return: ДФ с фичами\n        \"\"\"\n        # # если нет цифровых колонок --> заполним их\n        # if self.category_columns and not self.numeric_columns:\n        #     self.numeric_columns = [col_name for col_name in tov.columns\n        #                             if col_name not in self.category_columns]\n        # если нет категориальных колонок --> заполним их\n        if self.numeric_columns and not self.category_columns:\n            self.category_columns = [col_name for col_name in df.columns\n                                     if col_name not in self.numeric_columns]\n\n        for col_name in self.category_columns:\n            if col_name in df.columns:\n                if self.use_catboost:\n                    df[col_name] = df[col_name].astype('category')\n                else:\n                    print(f'Трансформирую колонку: {col_name}')\n                    # Create dummy variables\n                    df = pd.get_dummies(df, columns=[col_name], drop_first=self.drop_first)\n\n                    self.new_columns.extend([col for col in df.columns\n                                             if col.startswith(col_name)])\n        return df\n\n    def apply_scaler(self, df):\n        \"\"\"\n        Масштабирование цифровых колонок\n        :param df: исходный ДФ\n        :return: нормализованный ДФ\n        \"\"\"\n        if not self.transform_columns:\n            self.transform_columns = self.numeric_columns\n        if self.scaler and self.transform_columns:\n            print(f'Применяю scaler: {self.scaler.__name__} '\n                  f'с аргументами: {self.args_scaler}')\n            args = self.args_scaler if self.args_scaler else tuple()\n            scaler = self.scaler(*args)\n            scaled_data = scaler.fit_transform(df[self.transform_columns])\n            if scaled_data.shape[1] != len(self.transform_columns):\n                print(f'scaler породил: {scaled_data.shape[1]} колонок')\n                new_columns = [f'pnf_{n:02}' for n in range(scaled_data.shape[1])]\n                df = pd.concat([df, pd.DataFrame(scaled_data, columns=new_columns)], axis=1)\n                self.exclude_columns.extend(self.transform_columns)\n            else:\n                df[self.transform_columns] = scaled_data\n\n            self.comment.update(scaler=self.scaler.__name__, args_scaler=self.args_scaler)\n        return df\n\n    @staticmethod\n    def make_name_columns(columns):\n        \"\"\"\n        Формирование наименований новых колонок, которые получаются после merge пары товаров\n        :param columns:\n        :return:\n        \"\"\"\n        new_columns = columns.copy()\n        for id in (1, 2):\n            new_columns.extend([f'{col}{id}' for col in columns])\n        return new_columns\n\n    @staticmethod\n    def cosine_distance(row):\n        a, b, distances = row['pic_emb1'], row['pic_emb2'], []\n        for a_idx, a_vec in enumerate(a):\n            if not isinstance(a_vec, numpy.ndarray) or a_vec.shape[0] != 128:\n                continue\n            a_vec = a_vec.reshape(1, -1)\n            for b_idx, b_vec in enumerate(b):\n                if not isinstance(b_vec, numpy.ndarray) or b_vec.shape[0] != 128:\n                    continue\n                b_vec = b_vec.reshape(1, -1)\n                distances.append((cosine_similarity(a_vec, b_vec)[0][0], a_idx, b_idx))\n        if not distances:\n            distances = [(row['cos_main_pic'], 0, 0)]\n        return distances\n\n    @staticmethod\n    def similar_pics(row):\n        \"\"\"\n        Получение ембендингов картинок с максимальным правдоподобием\n        :param row: строка ДФ\n        :return: ембендинги картинок\n        \"\"\"\n        a_idx, b_idx = row['cos_idxs']\n        return row['pic_emb1'][a_idx], row['pic_emb2'][b_idx]\n\n    @staticmethod\n    def compare_attributes(row):\n        \"\"\"\n        Сравнение характеристик в полях characteristic_attributes_mapping\n        :param row: строка ДФ\n        :return: коэффициент похожести\n        \"\"\"\n        att1, att2 = row['cam1'], row['cam2']\n        if isinstance(att1, dict) and isinstance(att2, dict):\n            founds = sum(v1[0].lower() == att2.get(k1, [''])[0].lower()\n                         for k1, v1 in att1.items())\n            return founds / max(map(len, (att1, att2)))\n        return 0\n\n    def parsing_charact(self, dict_1, dict_2):\n        \"\"\" Принимает два словаря и возвращает число одинаковых и разных ключей \"\"\"\n\n        def lists_intersection(list_1, list_2) -> float:\n            s1, s2 = set(list_1), set(list_2)\n            return len(s1.intersection(s2)) / len(s1.union(s2))\n\n        # функция по обработке ключа\n        def get_key_category(keys):\n            key1 = set(dict_1.keys()).intersection(keys)\n            key2 = set(dict_2.keys()).intersection(keys)\n\n            if not key1:\n                return 0\n            if not key2:\n                return 1\n            key1, key2 = key1.pop(), key2.pop()\n            if self.remap_dict[key1] in diff_1 or self.remap_dict[key2] in diff_2:\n                return 2\n            item_interseption = lists_intersection(dict_1[key1], dict_2[key2])\n            if item_interseption < 0.01:\n                return 3\n            if item_interseption > 0.99:\n                return 4\n            if item_interseption > 0.8:\n                return 5\n            if item_interseption > 0.65:\n                return 6\n            if item_interseption > 0.5:\n                return 7\n            return 8\n\n        def change_keys(key):\n            keys_data[self.importante_keys[key]] = get_key_category(key)\n\n        keys_1 = set([*map(lambda x: self.remap_dict.get(x, x), dict_1.keys())])\n        keys_2 = set([*map(lambda x: self.remap_dict.get(x, x), dict_2.keys())])\n        diff_1 = keys_1.difference(keys_2)\n        diff_2 = keys_2.difference(keys_1)\n\n        # формируем массив для хранения хар-ки ключей\n        keys_data = np.zeros(len(self.importante_keys))\n        list(map(change_keys, self.importante_keys))\n        return keys_data\n\n    def fit(self, df, pair, rebuilding_pairs=False, preprocess_to_csv=False):\n        \"\"\"\n        Формирование фич\n        :param df: ДФ с товарами\n        :param pair: ДФ с парами товаров\n        :param rebuilding_pairs: перестроить ДФ с парами товаров\n        :param preprocess_to_csv: сохранить предобработанный файл в .csv\n        :return: обработанный ДФ с парами\n        \"\"\"\n        # добавление категорийных колонок после merge пары товаров\n        if self.category_columns:\n            self.category_columns = self.make_name_columns(self.category_columns)\n\n        if rebuilding_pairs or not self.preprocess_path_file or (\n                self.preprocess_path_file and not self.preprocess_path_file.is_file()):\n            tqdm.pandas()\n\n            df_columns = df.columns.to_list()\n            columns = df_columns.copy()\n            if 'variantid' in columns:\n                columns.remove('variantid')\n\n            for id in (1, 2):\n                pair = pair.merge(df[df_columns], how='left', left_on=f'variantid{id}',\n                                  right_index=True)\n\n                # переименуем колонки в более удобный вид, чтобы знать чьи они\n                pair.rename(columns={col: f'{col}{id}' for col in columns}, inplace=True)\n\n            # категории одинаковые?\n            pair['eq_cats'] = (pair.categories1 == pair.categories2).astype(int)\n\n            # для прохождения процесса на х32 машинах\n            if PARQUET_ENGINE != 'fastparquet':\n                for col in ('name_bert', 'main_pic', 'name_pic', 'pic_emb_mean'):\n                    if col not in df_columns:\n                        continue\n                    print(f'\\nСчитаю косинусное расстояние для \"{col}\"')\n                    pair[f'cos_{col}'] = pair.progress_apply(\n                        lambda row: cosine_similarity([row[f'{col}1']],\n                                                      [row[f'{col}2']])[0][0], axis=1)\n\n                print('\\nСчитаю косинусное расстояние между парами pic_embeddings_resnet_v1')\n                pair['cos_pics'] = pair.progress_apply(lambda row: self.cosine_distance(row),\n                                                       axis=1)\n                pair['cos_values'] = pair['cos_pics'].apply(\n                    lambda r: [*map(lambda x: x[0], r)])\n                pair['cos_idxs'] = pair['cos_pics'].apply(lambda x: max(x)[1:])\n                pair['cos_pic_min'] = pair['cos_values'].map(np.min)\n                pair['cos_pic_max'] = pair['cos_values'].map(np.max)\n                pair['cos_pic_mean'] = pair['cos_values'].map(np.mean)\n                pair['epic1'] = pair.apply(lambda row: self.similar_pics(row), axis=1)\n                pair['epic2'] = pair['epic1'].map(lambda x: x[1])\n                pair['epic1'] = pair['epic1'].map(lambda x: x[0])\n\n            # тут считается коэффициент сравнение текстов\n            for col in ('name_stem', 'name_lemm', 'name_stem_lemm', 'cam_stem', 'cam_lemm'):\n                if col not in df_columns:\n                    continue\n                print(f'\\nСчитаю расстояние Левенштейна для \"{col}\"')\n                pair[f'fuz_{col}'] = pair.progress_apply(\n                    lambda row: fuzz.token_sort_ratio(row[f'{col}1'], row[f'{col}2']) / 100,\n                    axis=1)\n\n            print(df_columns)\n            print()\n            print(pair.columns.to_list())\n\n            # тут считается коэффициент отношения количества слов в наименовании товара\n            calc_fract = lambda a, b: 1 - abs(a - b) / max(a, b)\n            # и количества характеристик\n            for col in ('name_st_len', 'name_lm_len', 'name_st_lm_len',\n                        'cam_len', 'cam_st_len', 'cam_lm_len'):\n                if col not in df_columns:\n                    continue\n                pair[f'fract_{col}'] = pair.apply(lambda row: calc_fract(row[f'{col}1'],\n                                                                         row[f'{col}2']),\n                                                  axis=1)\n\n            # Вычисления коэффициента похожести характеристик\n            print(f'\\nСчитаю коэффициент похожести характеристик')\n            pair['cam_like'] = pair.progress_apply(lambda row: self.compare_attributes(row),\n                                                   axis=1)\n\n            # обработка колонок с характеристиками товара\n            atr_columns = [*filter(lambda x: re.fullmatch(r'atr_\\d+', x), df_columns)]\n            for col in atr_columns:\n                print(f'\\nСчитаю расстояние Левенштейна для \"{col}\"')\n                pair[f'fuz_{col}'] = pair.progress_apply(\n                    lambda row: fuzz.token_sort_ratio(row[f'{col}1'], row[f'{col}2']) / 100,\n                    axis=1)\n\n            print('\\nФормирование сравнений атрибутов характеристик')\n            atr_columns = [*filter(lambda x: re.fullmatch(r'atr_\\d+', x), pair.columns)]\n            atr_columns = sorted(set([*map(lambda x: x[:6], atr_columns)]))\n            for col in atr_columns:\n                pair[f'comp_{col}'] = (pair[f'{col}1'] == pair[f'{col}2']).astype(int)\n\n            if 'target' in pair.columns:\n                pair['target'] = pair['target'].astype(int)\n                # маркировка категорий \"3\" как в бейзлайне\n                edge = 1000 if len(pair) > 20_000 else 50\n                cat3_counts = pair[\"cat31\"].value_counts().to_dict()\n                pair[\"cat3_grouped\"] = pair[\"cat31\"].apply(\n                    lambda x: x if cat3_counts[x] > edge else \"rest\")\n            else:\n                cat3_grouped = pair[\"cat3_grouped\"]\n                pair.drop(\"cat3_grouped\", axis=1, inplace=True)\n                pair[\"cat3_grouped\"] = cat3_grouped\n\n            grp_keys = pd.read_excel('grp_key.xlsx')\n            grp_keys['key_unique'] = grp_keys['key_unique'].str.replace(\"' '\", \"', '\")\n            grp_keys['key_unique'] = grp_keys['key_unique'].map(literal_eval)\n\n            # построение словаря ключ в поле cam1 или cam2 в подменных ключ\n            self.remap_dict = {}\n            for idx, row in grp_keys.iterrows():\n                for key in row['key_unique']:\n                    self.remap_dict[key] = row['key']\n\n            self.importante_keys = dict(map(lambda x: [tuple(x[1]), x[0]],\n                                            enumerate(grp_keys.key_unique.to_list())))\n            key_cols = [f'keys_{i + 1}' for i in range(len(self.importante_keys))]\n\n            print('\\nПостроение пересечения ключей в словарях характеристик товаров')\n\n            pair[key_cols] = pair[['cam1', 'cam2']].progress_apply(\n                lambda x: pd.Series(self.parsing_charact(*x)), axis=1).astype(int).astype(\n                'category')\n\n            print('--------------Информация о парах товаров-------------------')\n            print(pair.info())\n            print('--------------Информация о парах товаров-------------------')\n\n            if self.preprocess_path_file:\n                start_time = print_msg(f'Сохраняю {self.preprocess_path_file.name}')\n                pair.to_pickle(self.preprocess_path_file)\n                if preprocess_to_csv:\n                    pair.to_csv(self.preprocess_path_file.with_suffix('.csv'), sep=';',\n                                index=False)\n                print_time(start_time)\n        else:\n            start_time = print_msg(f'Читаю {self.preprocess_path_file.name}')\n            pair = pd.read_pickle(self.preprocess_path_file)\n            print_time(start_time)\n\n        return pair\n\n    def transform(self, df, pair, model_columns=None, expand_columns=[]):\n        \"\"\"\n        Формирование остальных фич\n        :param df: ДФ с товарами\n        :param pair: ДФ с парами товаров\n        :param model_columns: список колонок, которые будут использованы в модели\n        :param expand_columns: список колонок с массивами, которые нужно поделить на колонки\n        :return: ДФ с фичами\n        \"\"\"\n        tqdm.pandas()\n\n        self.exclude_columns = self.make_name_columns(self.exclude_columns)\n        self.exclude_columns.extend(['cat3_x', 'cat3_y'])\n\n        # выделение эмбендингов в отдельные колонки\n        for c_name in expand_columns:\n            pair = self.expand_column(pair, c_name)\n\n        if model_columns is None:\n            model_columns = [col for col in pair.columns.to_list() if\n                             col not in self.exclude_columns]\n\n        pair = self.cat_dummies(pair)\n\n        pair = self.apply_scaler(pair)\n\n        model_columns.extend(self.new_columns)\n\n        exclude_columns = [col for col in self.exclude_columns if col in pair.columns]\n        exclude_columns.extend(col for col in pair.columns if col not in model_columns)\n\n        cam_atrs = [*filter(lambda x: re.fullmatch(r'atr_\\d+', x), pair.columns)]\n        fuz_atts = [*filter(lambda x: re.fullmatch(r'fuz_atr_\\d+', x), pair.columns)]\n        comp_atrs = [*filter(lambda x: re.fullmatch(r'comp_atr_\\d+', x), pair.columns)]\n        keys_atrs = [*filter(lambda x: re.fullmatch(r'keys_\\d+', x), pair.columns)]\n\n        # добавим в исключенные колонки атрибуты характеристик товара\n        # exclude_columns.extend(cat_atrs)\n\n        # добавим в категорийные колонки атрибуты характеристик товара\n\n        if exclude_columns:\n            pair.drop(exclude_columns, axis=1, inplace=True)\n\n        self.exclude_columns = exclude_columns\n\n        self.category_columns = [col for col in self.category_columns if col in pair.columns]\n\n        # Переводим типы данных в минимально допустимые - экономим ресурсы\n        pair = memory_compression(pair, use_category=False)\n\n        if 'target' in pair.columns:\n            self.pair_columns = pair.columns.to_list()\n            self.pair_columns.remove('target')\n            self.pair_columns.sort()\n            pair = pair[['target'] + self.pair_columns]\n        else:\n            pair = pair[self.pair_columns]\n\n        return pair\n\n    def add_revers_pairs(self, pair, test_df, number_fractions=15, all_rows=True):\n        \"\"\"\n        Добавление перевернутых пар в тренировочную выборку с target=1\n        :param pair: тренировочный ДФ\n        :param test_df: тестовый ДФ\n        :param number_fractions: количество категорий для перевернутых пар 1-40\n        :param all_rows: добавлять все строки перевернутых пар\n        :return: ДФ с добавленными перевернутыми парами\n        \"\"\"\n        # Первые 15 имеют относительную долю на тесте более чем в 2 раза выше,\n        # для остальных 1 < доля < 2\n\n        pair_columns = pair.columns.to_list()\n\n        var1cols, var2cols = [], []\n\n        pre_columns = ['categories', 'cat4_cnt', 'color', 'cam', 'cat3', 'cat3_cnt',\n                       'cat',\n                       'name_st_len', 'name_lm_len', 'cam_len', 'cam_st_len',\n                       'name_st_lm_len']\n\n        expand_columns = ['main_pic', 'name_bert', 'name_pic', 'epic', 'pic_emb_mean']\n\n        for column_name in expand_columns:\n            for id, variant in zip((1, 2), (var1cols, var2cols)):\n                c_name = f'{column_name}{id}'\n                c_pref = ''.join(map(lambda s: s[0], c_name.split('_'))) + c_name[-1]\n                variant.append(c_pref)\n        var1patt = '|'.join(var1cols)\n        var2patt = '|'.join(var2cols)\n\n        var1columns = [*filter(lambda x: re.fullmatch(fr'^(?:{var1patt})_\\d+', x),\n                               pair_columns)]\n        var2columns = [*filter(lambda x: re.fullmatch(fr'^(?:{var2patt})_\\d+', x),\n                               pair_columns)]\n\n        for id, var_cols in zip((1, 2), (var1columns, var2columns)):\n            var_cols.extend([*filter(lambda x: re.fullmatch(fr'^atr_\\d+{id}$', x),\n                                     pair_columns)])\n            var_cols.extend([*map(lambda x: f'{x}{id}', pre_columns)])\n\n        var1columns = sorted(filter(lambda x: x in pair_columns, var1columns))\n        var2columns = sorted(filter(lambda x: x in pair_columns, var2columns))\n\n        #\n        grp1 = pair.groupby('cat31').agg(train_count=('variantid1', 'count'))\n        grp2 = test_df.groupby('cat31').agg(test_count=('variantid1', 'count'))\n        grp1['train_prs'] = grp1['train_count'] / len(pair)\n        grp2['test_prs'] = grp2['test_count'] / len(test_df)\n        grp = grp1.merge(grp2, left_index=True, right_index=True, how='left')\n        grp.fillna(0, inplace=True)\n        grp['diff'] = grp['test_prs'] - grp['train_prs']\n        grp['divide'] = grp['test_prs'] / grp['train_prs']\n        grp.sort_values(['divide', 'test_prs'], ascending=[False, True], inplace=True)\n        # grp.to_excel('grp_cat31.xlsx')\n        grp = grp[grp['divide'] > 1]\n        grp.reset_index(inplace=True)\n\n        grp = grp.iloc[:number_fractions, :]\n        # print(grp[['cat31', 'train_prs', 'test_prs', 'divide']])\n\n        if number_fractions == -999:\n            self.comment.update(reverse_cats='all_rows')\n            add_pairs = pair[pair.target > 0]\n            temp = add_pairs[var1columns]\n            add_pairs[var1columns] = add_pairs[var2columns]\n            add_pairs[var2columns] = temp\n        elif all_rows:    \n            self.comment.update(reverse_cats=number_fractions)\n            add_pairs = pair[(pair.target > 0) & (pair.cat32.isin(grp.cat31.to_list()))]\n            temp = add_pairs[var1columns]\n            add_pairs[var1columns] = add_pairs[var2columns]\n            add_pairs[var2columns] = temp\n        else:\n            self.comment.update(reverse_cats_auto=number_fractions)\n            add_pairs = None\n            for idx, row in grp.iterrows():\n                likes = pair[(pair.target > 0) & (pair.cat32 == row.cat31)]\n                # вычисляем количество строк для балансировки категорий\n                likes = likes.head(int(row['train_count'] * (row['divide'] - 1)) + 1)\n                print(f'Категория: {row.cat31} добавлено записей: {len(likes)}')\n\n                temp = likes[var1columns]\n                likes[var1columns] = likes[var2columns]\n                likes[var2columns] = temp\n                if add_pairs is None:\n                    add_pairs = likes\n                else:\n                    add_pairs = pd.concat([add_pairs, likes], ignore_index=True)\n\n        pair = pd.concat([pair, add_pairs], ignore_index=True)\n        print(f'В тренировочную выборку добавлено записей: {len(add_pairs)}')\n        add_pairs = temp = likes = None\n        gc.collect()\n        if self.use_catboost:\n            for col_name in self.category_columns:\n                if col_name in pair.columns:\n                    pair[col_name] = pair[col_name].astype('category')\n        return pair\n\n    def fit_transform(self, df, pair, rebuilding_pairs=False, preprocess_to_csv=False,\n                      model_columns=None, expand_columns=[]):\n        \"\"\"\n        fit + transform\n        :param df: ДФ с товарами\n        :param pair: ДФ с парами товаров\n        :param rebuilding_pairs: перестроить ДФ с парами товаров\n        :param preprocess_to_csv: сохранить предобработанный файл в .csv\n        :param model_columns: список колонок, которые будут использованы в модели\n        :param expand_columns: список колонок с массивами, которые нужно поделить на колонки\n        :return: ДФ с фичами\n        \"\"\"\n        pair = self.fit(df, pair,\n                        rebuilding_pairs=rebuilding_pairs,\n                        preprocess_to_csv=preprocess_to_csv)\n        pair = self.transform(df, pair,\n                              model_columns=model_columns,\n                              expand_columns=expand_columns)\n        return pair\n\n    @staticmethod\n    def expand_column(df, column_name):\n        \"\"\"\n        Преобразование массива в одной колонке в кучу колонок по числу колонок массива\n        :param df: ДФ\n        :param column_name: столбец ФД\n        :return: преобразованный ДФ\n        \"\"\"\n        for id in (1, 2):\n            col_name = f'{column_name}{id}'\n            col_pref = ''.join(map(lambda s: s[0], col_name.split('_'))) + col_name[-1]\n            len_name = len(df.iloc[0, df.columns.to_list().index(col_name)])\n            new_cols = [f'{col_pref}_{i}' for i in range(len_name)]\n            df[new_cols] = df[col_name].values.tolist()\n            df.drop(col_name, axis=1, inplace=True)\n        return df\n\n    def __getstate__(self) -> dict:  # Как мы будем \"сохранять\" класс\n        return self.__dict__\n\n    def __setstate__(self, state: dict):  # Как мы будем восстанавливать класс из байтов\n        self.__dict__ = state","metadata":{"execution":{"iopub.status.busy":"2023-05-30T15:19:43.033708Z","iopub.execute_input":"2023-05-30T15:19:43.034542Z","iopub.status.idle":"2023-05-30T15:19:43.191186Z","shell.execute_reply.started":"2023-05-30T15:19:43.034514Z","shell.execute_reply":"2023-05-30T15:19:43.190327Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"MODELS_LOGS = WORK_PATH.joinpath('scores.logs')\n\ntrain_pairs = DATASET_PATH.joinpath('train_pairs.parquet')\ntest_pairs = DATASET_PATH.joinpath('test_pairs_wo_target.parquet')\nfile_train = DATASET_PATH.joinpath('train_data.parquet')\nfile_test = DATASET_PATH.joinpath('test_data.parquet')\n\n\ndef predict_auc_macro(df: pd.DataFrame, prec_level: float = 0.75,\n                      cat_column: str = \"cat3_grouped\") -> float:\n    y_true = df[\"target\"]\n    y_pred = df[\"scores\"]\n    categories = df[cat_column]\n\n    weights = []\n    pr_aucs = []\n\n    unique_cats, counts = np.unique(categories, return_counts=True)\n\n    for i, category in enumerate(unique_cats):\n        cat_idx = np.where(categories == category)[0]\n        y_pred_cat = y_pred.iloc[cat_idx]\n        y_true_cat = y_true.iloc[cat_idx]\n\n        y, x, thr = precision_recall_curve(y_true_cat, y_pred_cat)\n        gt_prec_level_idx = np.where(y >= prec_level)[0]\n\n        try:\n            pr_auc_prec_level = auc(x[gt_prec_level_idx], y[gt_prec_level_idx])\n            if not np.isnan(pr_auc_prec_level):\n                pr_aucs.append(pr_auc_prec_level)\n                weights.append(counts[i] / len(categories))\n        except ValueError as err:\n            pr_aucs.append(0)\n            weights.append(0)\n    return np.average(pr_aucs, weights=weights)\n\n\ndef read_train_test_sample(rebuilding_pairs=False, rows=500):\n    file_preprocess = WORK_PATH.joinpath('preprocess_sample.pkl')\n    dts_object = WORK_PATH.joinpath('dts_object_sample.pkl')\n    files = (WORK_PATH.joinpath('test_smp.pkl'), WORK_PATH.joinpath('train_smp.pkl'))\n    start_time = print_msg('Готовлю небольшую выборку с парами')\n\n    # читаем списки товаров в один ДФ\n    read_sample = False\n    tov = pd.concat([pd.read_parquet(file, engine=PARQUET_ENGINE)\n                     for file in (file_train, file_test)[read_sample:]],\n                    ignore_index=True)\n    tov.drop_duplicates(['variantid'], inplace=True)\n\n    dpr = DataPreprocess()\n\n    train = pd.read_parquet(train_pairs, engine=PARQUET_ENGINE)\n    tov_train, train = dpr.make_sample(tov, train, rows=rows)\n    tov_train = dpr.fit_transform(tov_train)\n    test = pd.read_parquet(test_pairs, engine=PARQUET_ENGINE)\n    tov_test, test = dpr.make_sample(tov, test, rows=rows // 2)\n    tov_test = dpr.fit_transform(tov_test)\n\n    tov = pd.concat([tov_train, tov_test])\n\n    # tov.to_pickle(file_preprocess)\n    # tov.to_csv(file_preprocess.with_suffix('.csv'), sep=';')\n    \n    cat_columns = ['categories', 'color']\n    dts = DataTransform(category_columns=cat_columns)\n    dts.exclude_columns = ['name', 'color', 'pic_emb', 'ch_att_map',\n                           'cam',\n                           'cat',\n                           # 'cat3',\n                           'name_pic',\n                           'main_pic',\n                           'cos_pics', 'cos_values', 'cos_idxs',\n                           'name_norm1', 'name_stem1', 'name_lemm1',\n                           'name_norm2', 'name_stem2', 'name_lemm2',\n                           'cam_norm1', 'cam_stem1', 'cam_lemm1',\n                           'cam_norm2', 'cam_stem2', 'cam_lemm2',\n                           ]    \n\n    for idx, dfp in enumerate((train, test)):\n        # print(dfp.info())\n\n        file = files[idx]\n        dts.preprocess_path_file = WORK_PATH.joinpath(f'preprocess_{file.name}')\n\n        dfp = dts.fit_transform(tov, dfp,\n                                rebuilding_pairs=rebuilding_pairs,\n                                preprocess_to_csv=True,\n                                # expand_columns=['name_bert', 'epic']\n                                )\n\n        print('tov.columns:', dfp.columns.to_list())\n        print('numeric_columns:', dts.numeric_columns)\n        print('category_columns:', dts.category_columns)\n        print('exclude_columns:', dts.exclude_columns)\n        dfp.to_pickle(files[idx])\n        with open(dts_object, \"wb\") as file_object:\n            pickle.dump(dts, file_object)\n\n    train = pd.read_pickle(files[0])\n    test = pd.read_pickle(files[1])\n    with open(dts_object, \"rb\") as file_object:\n        dts = pickle.load(file_object)\n\n    print_time(start_time)\n    return train, test, dts\n\n\ndef read_train_test(rebuilding_pairs=False, remake_train_test_files=False):\n    \"\"\"\n    Чтение подготовленных данных, если файлы отсутствуют - выполняется предобработка\n    :param rebuilding_pairs: полностью перестроить ДФ с парами train и test\n    :param remake_train_test_files: пересобрать ДФ с парами train и test\n    :return:\n    \"\"\"\n    file_preprocess = DATASET_PATH.joinpath('preprocess_data.pkl')\n    dts_object = DATASET_PATH.joinpath('dts_object.pkl')\n\n    need_make_files = True\n    if file_preprocess.is_file():\n        # если подготовленных файлов нет - читаем предобработанный ДФ и готовим их\n        if rebuilding_pairs or not all(file.with_suffix('.pkl').is_file()\n                                       for file in (train_pairs, test_pairs)):\n            start_time = print_msg('Читаю предобработанный ДФ')\n            tov = pd.read_pickle(file_preprocess)\n        else:\n            need_make_files = False\n    else:\n        start_time = print_msg('Готовлю датасет с товарами')\n        # читаем списки товаров в один ДФ\n        read_sample = False\n        tov = pd.concat([pd.read_parquet(file, engine=PARQUET_ENGINE)\n                         for file in (file_train, file_test)[read_sample:]],\n                        ignore_index=True)\n\n        tov.drop_duplicates(['variantid'], inplace=True)\n\n        dpr = DataPreprocess()\n        tov = dpr.fit_transform(tov)\n\n        print('Сохраняю предобработанный ДФ')\n        tov.to_pickle(WORK_PATH.joinpath('preprocess_data.pkl'))\n        print_time(start_time)\n\n        print(tov.info())\n        print(tov.columns.to_list())\n\n    if need_make_files or remake_train_test_files:\n\n        if remake_train_test_files:\n            tov = None\n            rebuilding_pairs = False\n            \n        cat_columns = ['categories', 'color']\n        dts = DataTransform(category_columns=cat_columns)\n        dts.exclude_columns = ['name', 'color', 'pic_emb', 'ch_att_map',\n                               'cam',\n                               'cat',\n                               # 'cat3',\n                               'name_pic',\n                               # 'main_pic',\n                               # 'name_bert',\n                               'cos_pics', 'cos_values', 'cos_idxs',\n                               'name_norm1', 'name_stem1', 'name_lemm1',\n                               'name_norm2', 'name_stem2', 'name_lemm2',\n                               'cam_norm1', 'cam_stem1', 'cam_lemm1',\n                               'cam_norm2', 'cam_stem2', 'cam_lemm2',\n                               ]            \n\n        for idx, file in enumerate((train_pairs, test_pairs)):\n            if (rebuilding_pairs or remake_train_test_files\n                    or not file.with_suffix('.pkl').is_file()):\n                start_time = print_msg(f\"Готовлю {('тренировочный', 'тестовый')[idx]} \"\n                                       f\"датасет\")\n                dfp = pd.read_parquet(file, engine=PARQUET_ENGINE)\n\n                print(dfp.info())\n\n                dts.preprocess_path_file = WORK_PATH.joinpath(\n                    f'preprocess_{file.name}').with_suffix('.pkl')\n\n                dfp = dts.fit(tov, dfp, rebuilding_pairs=rebuilding_pairs)\n\n                dfp = dts.transform(tov, dfp)\n\n                print('tov.columns:', dfp.columns.to_list())\n                print('numeric_columns:', dts.numeric_columns)\n                print('category_columns:', dts.category_columns)\n                print('exclude_columns:', dts.exclude_columns)\n                dfp.to_pickle(WORK_PATH.joinpath(file.name).with_suffix('.pkl'))\n                print(dfp.info())\n\n                with open(dts_object, \"wb\") as file_object:\n                    pickle.dump(dts, file_object)\n\n                print_time(start_time)\n\n        work_path = WORK_PATH\n    else:\n        work_path = DATASET_PATH\n\n    start_time = print_msg('Читаю датасеты с парами товаров')\n\n    train = pd.read_pickle(work_path.joinpath(train_pairs.name).with_suffix('.pkl'))\n    test = pd.read_pickle(work_path.joinpath(test_pairs.name).with_suffix('.pkl'))\n\n    cat_columns = ['cat3', 'categories', 'color']\n    dts = DataTransform(category_columns=cat_columns)\n    dts.exclude_columns = ['name', 'color', 'pic_emb', 'ch_att_map',\n                           'cam',\n                           'cat',\n                           'name_pic',\n                           # 'main_pic',\n                           # 'name_bert',\n                           'cos_pics', 'cos_values', 'cos_idxs',\n                           'name_norm1', 'name_stem1', 'name_lemm1',\n                           'name_norm2', 'name_stem2', 'name_lemm2',\n                           'cam_norm1', 'cam_stem1', 'cam_lemm1',\n                           'cam_norm2', 'cam_stem2', 'cam_lemm2',\n                           'name_stem_lemm1', 'name_stem_lemm2',\n                           ]\n\n    print_time(start_time)\n    return train, test, dts\n\n\ndef get_max_num(file_logs=None):\n    \"\"\"Получение максимального номера итерации обучения моделей\n    :param file_logs: имя лог-файла с полным путем\n    :return: максимальный номер\n    \"\"\"\n    if file_logs is None:\n        file_logs = MODELS_LOGS\n\n    if not file_logs.is_file():\n        with open(file_logs, mode='a') as log:\n            log.write('num;mdl;roc_auc;acc_train;acc_valid;sc_train;score;WF1;'\n                      'model_columns;exclude_columns;cat_columns;comment\\n')\n        max_num = 0\n    else:\n        df = pd.read_csv(file_logs, sep=';')\n        df.num = df.index + 1\n        max_num = df.num.max()\n    return max_num\n\n\ndef predict_train_valid(model, datasets, label_enc=None, max_num=0):\n    \"\"\"Расчет метрик для модели: accuracy на трейне, на валидации, на всем трейне, roc_auc\n    и взвешенная F1-мера на валидации\n    :param model: обученная модель\n    :param datasets: кортеж с тренировочной, валидационной и полной выборками\n    :param label_enc: используемый label_encоder для target'а\n    :param max_num: максимальный порядковый номер обучения моделей\n    :return: accuracy на трейне, на валидации, на всем трейне, roc_auc и взвешенная F1-мера\n    \"\"\"\n    X_train, X_valid, y_train, y_valid, train, target, test_df, trn = datasets\n    valid_pred = model.predict(X_valid)\n    train_pred = model.predict(X_train)\n    train_full = model.predict(train)\n\n    predict_train = model.predict_proba(X_train)[:, 1]\n    # выберем строки по индексам обучающей выборки\n    df = trn.loc[X_train.index]\n    df[\"scores\"] = predict_train\n    score_train = predict_auc_macro(df)\n\n    predict_proba = model.predict_proba(X_valid)[:, 1]\n    # выберем строки по индексам валидационной выборки\n    df = trn.loc[X_valid.index]\n    df[\"scores\"] = predict_proba\n    score_valid = predict_auc_macro(df)\n\n    precision, recall, thrs = precision_recall_curve(df[\"target\"], df[\"scores\"])\n    pr_auc = auc(recall, precision)\n    fig, ax1 = plt.subplots(1, figsize=(15, 7))\n    ax1.plot(recall, precision)\n    ax1.axhline(y=0.75, color='grey', linestyle='-')\n    # сохранение картинки\n    file_to_save = f'pr_auc_{max_num:03}.png'\n    plt.savefig(PREDICTIONS_DIR.joinpath(file_to_save))\n    # plt.show()\n\n    f1w = f1_score(y_valid, valid_pred)\n    acc_valid = accuracy_score(y_valid, valid_pred)\n    acc_train = accuracy_score(y_train, train_pred)\n    acc_full = accuracy_score(target, train_full)\n    roc_auc = roc_auc_score(y_valid, predict_proba)\n\n    print(f'Score auc_macro = train:{score_train:.6f} valid:{score_valid:.6f}')\n    print(f'Accuracy train:{acc_train} valid:{acc_valid} roc_auc:{roc_auc} F1:{f1w:.6f}')\n\n    return acc_train, acc_valid, roc_auc, f1w, score_train, score_valid\n\n\ndef predict_test(idx_fold, model, datasets, max_num=0, submit_prefix='lg_', label_enc=None,\n                 save_predict_proba=True):\n    \"\"\"Предсказание для тестового датасета.\n    Расчет метрик для модели: accuracy на трейне, на валидации, на всем трейне, roc_auc\n    и взвешенная F1-мера на валидации\n    :param idx_fold: номер фолда при обучении\n    :param model: обученная модель\n    :param datasets: кортеж с тренировочной, валидационной и полной выборками\n    :param max_num: максимальный порядковый номер обучения моделей\n    :param submit_prefix: префикс для файла сабмита для каждой модели свой\n    :param label_enc: используемый label_encоder для target'а\n    :param save_predict_proba: сохранять файл с вероятностями предсказаний\n    :return: accuracy на трейне, на валидации, на всем трейне, roc_auc и взвешенная F1-мера\n    \"\"\"\n    X_train, X_valid, y_train, y_valid, train, target, test_df, trn = datasets\n    # постфикс если было обучение на отдельных фолдах\n    nfld = f'_{idx_fold}' if idx_fold else ''\n    columns_no_to_model = ['target', 'variantid1', 'variantid2']\n    predictions = model.predict(test_df.drop(columns_no_to_model[1:], axis=1))\n    predict_train = model.predict(train)\n\n    if label_enc:\n        predictions = label_enc.inverse_transform(predictions)\n        predict_train = label_enc.inverse_transform(predict_train)\n\n    # печать размерности предсказаний и списка меток классов\n    classes = model.classes_.tolist()\n    print('predict_proba.shape:', predictions.shape, 'classes:', classes)\n\n    predict_proba = model.predict_proba(test_df.drop(columns_no_to_model[1:], axis=1))\n    train_proba = model.predict_proba(train)\n\n    submit_csv = f'{submit_prefix}submit_{max_num:03}{nfld}.csv'\n    file_submit_csv = PREDICTIONS_DIR.joinpath(submit_csv)\n    file_proba_csv = PREDICTIONS_DIR.joinpath(submit_csv.replace('submit_', 'proba_'))\n    file_train_csv = PREDICTIONS_DIR.joinpath(submit_csv.replace('submit_', 'train_'))\n\n    # Сохранение предсказаний в файл\n    submit = test_df[columns_no_to_model[1:] + ['cat3_grouped']]\n    submit['target'] = predict_proba[:, 1]\n    submit.to_csv(file_submit_csv, index=False)\n    if save_predict_proba:\n        train_sp = trn[columns_no_to_model]\n        train_sp['pred_target'] = predict_train\n        train_sp.to_csv(file_train_csv, index=False)\n\n        try:\n            train_sp[classes] = train_proba\n            train_sp.to_csv(file_train_csv, index=False)\n        except:\n            pass\n\n    acc_tr, acc_val, roc_auc, f1w, sc_tr, sc_val = predict_train_valid(model,\n                                                                       datasets,\n                                                                       label_enc=label_enc,\n                                                                       max_num=max_num)\n    return acc_tr, acc_val, roc_auc, f1w, sc_tr, sc_val\n\n\ndef add_features_to_preprocessed_file():\n    file_name = 'preprocess_data.pkl'\n    file_preprocess = DATASET_PATH.joinpath(file_name)\n\n    start_time = print_msg(f'Читаю предобработанный ДФ: {file_name}')\n    if file_preprocess.is_file():\n        tov = pd.read_pickle(file_preprocess)\n        print(tov.columns.to_list())\n    else:\n        print('Ошибка!!! Предобработанный файл не обнаружен: запустите read_train_test()')\n        return\n\n    print_time(start_time)\n\n    dpr = DataPreprocess()\n\n    tqdm.pandas()\n\n    print('Считаю частотную зависимость атрибутов')\n    # подсчет частотности использования характеристик\n    threshold = 0.01  # порог, ниже которого характеристика не важна\n    max_features = 99  # максимальное количество атрибутов\n    attr_keys = tov['cam'].map(lambda x: set(x.keys())).to_list()\n    keys_set = set([key for keys in attr_keys for key in keys])\n    keys_dict, len_tov = {}, len(tov['cam'])\n    for key in tqdm(keys_set):\n        keys_dict[key] = sum(tov['cam'].apply(lambda x: len(x.get(key, '')))) / len_tov\n\n    print('Всего уникальных атрибутов =', len(keys_dict))\n\n    keys_df = pd.DataFrame(keys_dict.items(), columns=['cam_key', 'ratio'])\n    keys_df.sort_values('ratio', ascending=False, inplace=True, ignore_index=True)\n    keys_df.to_excel('cam_key.xlsx', index=False)\n\n    keys_df['key'] = keys_df.cam_key.map(lambda x: x.split(',')[0])\n    grp = keys_df.groupby('key', as_index=False).agg(\n        key_unique=('cam_key', lambda x: x.unique()),\n        ratio=('ratio', 'sum')\n    )\n    grp.sort_values('ratio', ascending=False, inplace=True, ignore_index=True)\n    grp = grp[grp.ratio >= threshold]\n    grp = grp.iloc[:max_features]\n    grp.to_excel('grp_key.xlsx', index=False)\n\n    stm = TextLemmatization()\n\n    print('Нормализация атрибутов')\n\n    from_list = lambda x: ' '.join(map(str.strip, x)).lower()\n\n    # добавление колонок со значениями атрибутов к ДФ с товаром\n    for idx, row in tqdm(grp.iterrows()):\n        attribute_name = f'atr_{idx + 1:02}'\n        # print(attribute_name, row['key'], row['key_unique'], row['key_unique'].shape)\n        for idx_key, key in enumerate(row['key_unique']):\n            if not idx_key:\n                tov[attribute_name] = tov['cam'].map(lambda x: from_list(x.get(key, [''])))\n                tov[attribute_name] = tov[attribute_name].map(lambda x: x if x else np.NaN)\n            else:\n                tov['temp'] = tov['cam'].map(lambda x: from_list(x.get(key, [''])))\n                tov['temp'] = tov['temp'].map(lambda x: x if x else np.NaN)\n                tov[attribute_name].fillna(tov['temp'], inplace=True)\n\n            if key in ('Гарантийный срок',):\n                tov[attribute_name] = tov[attribute_name].str.findall(r'\\d+')\n                tov[attribute_name] = tov[attribute_name].map(\n                    lambda x: x[0] if isinstance(x, list) and len(x) else np.NaN)\n\n        tov['temp'] = pd.to_numeric(tov[attribute_name], errors=\"coerce\")\n        atr_fill = len_tov - tov[attribute_name].isna().sum()\n        atr_nums = len_tov - tov['temp'].isna().sum()\n        # если количество цифровых значений больше 96% тогда преобразуем колонку в числа\n        if atr_nums > atr_fill * 0.96:\n            tov[attribute_name] = pd.to_numeric(tov[attribute_name], errors=\"coerce\")\n            tov[attribute_name].fillna(0, inplace=True)\n\n        else:\n            tov[attribute_name].fillna('None', inplace=True)\n            # if idx < 20:\n            #     tov[attribute_name] = tov[attribute_name].map(stm.normalize_text)\n            #     tov[attribute_name] = tov[attribute_name].map(\n            #         lambda x: ' '.join(stm.stem_text(x)))\n            tov[attribute_name] = tov[attribute_name].map(stm.normalize_text)\n            tov[attribute_name] = tov[attribute_name].str.join(' ')\n\n    tov.drop('temp', axis=1, inplace=True)\n\n    print('Сохраняю предобработанный ДФ')\n    tov.to_pickle(WORK_PATH.joinpath(file_name))\n    print_time(start_time)\n\n\ndef add_features_to_pairs_preprocessed():\n    file_name = 'preprocess_data.pkl'\n    file_preprocess = DATASET_PATH.joinpath(file_name)\n\n    start_time = print_msg(f'Читаю предобработанный ДФ: {file_name}')\n    if file_preprocess.is_file():\n        tov = pd.read_pickle(file_preprocess)\n        print(tov.columns.to_list())\n    else:\n        print('Ошибка!!! Предобработанный файл не обнаружен: запустите read_train_test()')\n        return\n\n    print_time(start_time)\n\n    tqdm.pandas()\n\n    dts = DataTransform()\n    \n    for idx, file in enumerate((train_pairs, test_pairs)):\n        start_time = print_msg(f\"Готовлю {('тренировочный', 'тестовый')[idx]} \"\n                               f\"датасет\")\n        dts.preprocess_path_file = WORK_PATH.joinpath(\n            f'preprocess_{file.name}').with_suffix('.pkl')\n\n        # читаем предобработанный файл\n        pair = dts.fit(None, None)\n\n        pair[\"cat3\"] = pair[\"cat1\"].apply(lambda x: x[\"3\"])\n\n        if 'target' in pair.columns:\n            pair['target'] = pair['target'].astype(int)\n            # маркировка категорий \"3\" как в бейзлайне\n            edge = 1000 if len(pair) > 20_000 else 50\n            cat3_counts = pair[\"cat3\"].value_counts().to_dict()\n            pair[\"cat3_grouped\"] = pair[\"cat3\"].apply(\n                lambda x: x if cat3_counts[x] > edge else \"rest\")\n        else:\n            cat3_grouped = pair[\"cat3_grouped\"]\n            pair.drop(\"cat3_grouped\", axis=1, inplace=True)\n            pair[\"cat3_grouped\"] = cat3_grouped\n\n        # # обработка только колонок с характеристиками товара\n        # atr_columns = [*filter(lambda x: re.fullmatch(r'atr_\\d+', x), tov.columns)]\n        #\n        # for id in (1, 2):\n        #     for col in atr_columns:\n        #         if f'{col}{id}' not in pair:\n        #             pair = pair.merge(tov[[col]], how='left', left_on=f'variantid{id}',\n        #                               right_index=True)\n        #             # переименуем колонки в более удобный вид, чтобы знать чьи они\n        #             pair.rename(columns={col: f'{col}{id}'}, inplace=True)\n        #\n        # for col in atr_columns:\n        #     # только для текстовых колонок\n        #     if str(tov[col].dtype) == 'object':\n        #         print(f'\\nСчитаю расстояние Левенштейна для \"{col}\"')\n        #         pair[f'fuz_{col}'] = pair.progress_apply(\n        #             lambda row: fuzz.token_sort_ratio(row[f'{col}1'], row[f'{col}2']) / 100,\n        #             axis=1)\n\n        print_time(start_time)\n        start_time = print_msg(f'Сохраняю {dts.preprocess_path_file.name}')\n        pair.to_pickle(dts.preprocess_path_file)\n\n        print_time(start_time)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T15:19:43.192688Z","iopub.execute_input":"2023-05-30T15:19:43.193052Z","iopub.status.idle":"2023-05-30T15:19:43.269724Z","shell.execute_reply.started":"2023-05-30T15:19:43.193023Z","shell.execute_reply":"2023-05-30T15:19:43.268829Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def objective(trial: optuna.Trial) -> float:\n    param = {\n        \"loss_function\": trial.suggest_categorical(\"loss_function\",\n                                                   [\"binary\"]),\n        \"auto_class_weights\": trial.suggest_categorical(\"auto_class_weights\",\n                                                        [None, \"Balanced\"]),\n        # \"iterations\": trial.suggest_int(\"iterations\", 200, 2000, step=200),\n        # \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        # \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.1, 0.2, step=0.05),\n    }\n\n    gbm = CatBoostClassifier(cat_features=cat_columns,\n                             eval_metric='TotalF1',\n                             early_stopping_rounds=80,\n                             random_seed=42,\n                             # task_type=\"GPU\",\n                             # devices='0:1',\n                             **param)\n\n    pruning_callback = CatBoostPruningCallback(trial, \"TotalF1\")\n\n    gbm.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        verbose=100,\n        early_stopping_rounds=80,\n        callbacks=[pruning_callback],\n    )\n\n    # evoke pruning manually.\n    pruning_callback.check_pruned()\n\n    accuracy = accuracy_score(y_valid, gbm.predict(X_valid))\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2023-05-30T15:19:43.271177Z","iopub.execute_input":"2023-05-30T15:19:43.271732Z","iopub.status.idle":"2023-05-30T15:19:43.279368Z","shell.execute_reply.started":"2023-05-30T15:19:43.271700Z","shell.execute_reply":"2023-05-30T15:19:43.278426Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"np.linspace(0.07, 0.18, 12)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T15:19:43.280843Z","iopub.execute_input":"2023-05-30T15:19:43.281196Z","iopub.status.idle":"2023-05-30T15:19:43.296676Z","shell.execute_reply.started":"2023-05-30T15:19:43.281167Z","shell.execute_reply":"2023-05-30T15:19:43.295625Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([0.07, 0.08, 0.09, 0.1 , 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17,\n       0.18])"},"metadata":{}}]},{"cell_type":"code","source":"import gc\n\nfile_logs_read = DATASET_PATH.joinpath('scores.logs')\nfile_logs = WORK_PATH.joinpath('scores.logs')\n\nif not file_logs.is_file():\n    df = pd.read_csv(file_logs_read, sep=';')\n    df.to_csv(file_logs, index=False, sep=';')\nelse:\n    df = pd.read_csv(file_logs, sep=';')\n\nif len(df):\n    df.num = df.index + 1\n    max_num = df.num.max()\nelse:    \n    max_num = 0\n\nstart_time = print_msg('Обучение Catboost классификатор...')\n\ndata_cls = DataTransform()\n\n# # небольшая выборка для тестов\n# train_df, test_df, data_cls = read_train_test_sample(rebuilding_pairs=True)\n\n# чтение подготовленного датасета\ntrain_df, test_df, dts = read_train_test(rebuilding_pairs=False)\n\n# print(train_df.shape, test_df.shape, set(train_df.columns) - set(test_df.columns),\n#       train_df.columns.to_list().index('cat3'))\n# print(train_df.columns.to_list())\n# print(test_df.columns.to_list())\n\ncam_atrs = sorted(filter(lambda x: re.fullmatch(r'atr_\\d+', x), train_df.columns))\nfuz_atts = sorted(filter(lambda x: re.fullmatch(r'fuz_atr_\\d+', x), train_df.columns))\ncomp_atrs = sorted(filter(lambda x: re.fullmatch(r'comp_atr_\\d+', x), train_df.columns))\nkeys_atrs = sorted(filter(lambda x: re.fullmatch(r'keys_\\d+', x), train_df.columns))\n\n# количество характеристик товара, которые будем использовать: задавать четное число\n# было 176\nnumbers_cam_atrs = 276  # 99 * 2\nnumbers_fuz_atrs = 276  # 99 * 2\nnumbers_comp_atrs = 276  # 99 * 2\nnumbers_keys_atrs = 276  # 99 * 2\n# перезапишем словарь из класса dts в класс data_cls\ndts.exclude_columns.extend(['pic_emb_mean', 'cam_lemm', 'name_stem_lemm',\n                            # 'cam_lm_len',  # эта колонка была в сабмите 207\n                            'cos_pics', 'epic'])\ndts.exclude_columns.extend(cam_atrs[numbers_cam_atrs:])\ndts.exclude_columns.extend(fuz_atts[numbers_fuz_atrs:])\ndts.exclude_columns.extend(comp_atrs[numbers_comp_atrs:])\ndts.exclude_columns.extend(keys_atrs[numbers_keys_atrs:])\n\ndts.exclude_columns.extend([\n    # 'fract_cam_lm_len',  # это было в сабмите 207\n    # 'fract_name_lm_len',  # это было в сабмите 207\n    # 'fuz_cam_lemm',  # это было в сабмите 207\n    # 'fuz_name_lemm',  # это было в сабмите 207\n    # 'name_lm_len',  # это было в сабмите 207\n    \n    # 'fuz_name_stem_lemm', # закоментил это\n    \n    # это новые колонки были в сабмите 56 - это раскоментарил\n    'fract_name_stem_lemm', \n    'frset_cam_stem', \n    'frset_name_lemm',  \n    'frset_name_stem', \n    'frset_name_stem_lemm', \n])\n\ndts.category_columns = ['cat3', 'cat31', 'cat32', 'categories', 'color']\n\n# if 'cat3' in dts.exclude_columns:\n#     dts.exclude_columns.remove('cat3')\n\n# удаление категорийных колонок, если они есть в исключенных\ncat_columns = dts.__dict__['category_columns']\ncat_columns = [col for col in cat_columns if col not in dts.exclude_columns]\ndts.__dict__['category_columns'] = cat_columns\n\n# cam_atrs_numeric = [*filter(lambda x: str(train_df[x].dtype)[:3] in ('int', 'flo'),\n#                             cam_atrs[:numbers_cam_atrs])]\n# cam_atrs = [*filter(lambda x: x not in cam_atrs_numeric, cam_atrs[:numbers_cam_atrs])]\n\ndata_cls.__dict__ = dts.__dict__.copy()\ndata_cls.category_columns.extend(['categories', 'categories1', 'categories2'])\ndata_cls.category_columns.append('cat3_grouped')\n# data_cls.category_columns.append('cat3')\ndata_cls.category_columns.extend(cam_atrs[:numbers_cam_atrs])\ndata_cls.category_columns.extend(keys_atrs[:numbers_keys_atrs])\n\n# print(train_df.columns.to_list())\n# print(data_cls.__dict__)\n\nstart_time = print_msg('Постобработка датасетов с парами товаров')\nexpand_columns = ['main_pic', 'name_bert']\ntrain_df = data_cls.transform(None, train_df, expand_columns=expand_columns)\ntest_df = data_cls.transform(None, test_df, expand_columns=expand_columns)\ntrain_df = data_cls.add_revers_pairs(train_df, test_df, number_fractions=-999, all_rows=True)\nprint_time(start_time)\n\nexclude_columns = dts.__dict__['exclude_columns']\nexclude_columns.extend([cn for cn in data_cls.exclude_columns if cn not in exclude_columns])\n\nif 'cat32' in train_df.columns:\n    train_df.drop('cat32', axis=1, inplace=True)\n    test_df.drop('cat32', axis=1, inplace=True)\n    exclude_columns.append('cat32')\n    if 'cat32' in data_cls.category_columns:\n        data_cls.category_columns.remove('cat32')\n\ndata_cls.exclude_columns = exclude_columns\n\ncat_columns = data_cls.category_columns\nmodel_columns = train_df.columns.to_list()\n\nprint('Обучаюсь на колонках:', model_columns)\nprint('Категорийные колонки:', cat_columns)\nprint('Исключенные колонки:', data_cls.exclude_columns)\n\nif 'cat31' in train_df.columns:\n    # добавление cat31 для валидации кто попался только один раз\n    for cat31 in train_df.cat31.unique():\n        if len(train_df.loc[train_df.cat31 == cat31]) < 5:\n            print(f'cat31 = {cat31}')\n            train_df = train_df.append(train_df.loc[train_df.cat31 == cat31])\n            train_df = train_df.append(train_df.loc[train_df.cat31 == cat31])\n            train_df = train_df.append(train_df.loc[train_df.cat31 == cat31])\n        elif len(train_df.loc[train_df.cat31 == cat31]) < 10:\n            print(f'cat31 = {cat31}')\n            train_df = train_df.append(train_df.loc[train_df.cat31 == cat31])\n\n    train_df.reset_index(drop=True, inplace=True)\n\nprint(f'Размер train_df = {train_df.shape}, test_df = {test_df.shape}')\n\ncolumns_no_to_model = ['target', 'variantid1', 'variantid2']\ntrn = train_df[columns_no_to_model + ['cat3_grouped', 'cat31']]\ntrain = train_df.drop(columns_no_to_model, axis=1)\ntarget = train_df['target']\n\n# print(train[cat_columns].dtypes)\n\n# test_sizes = (0.1, 0.14, 0.15, 0.15000000000000002)\n# test_sizes = (0.1, 0.12, 0.14, 0.15)\ntest_sizes = (0.13,)\n# test_sizes = (0.1,)\n# test_sizes = np.linspace(0.1, 0.4, 4)\n# for num_iters in range(50, 151, 50):\n# for SEED in range(100):\nfor test_size in test_sizes:\n    # for num_iters in range(200, 901, 50):\n    # for num_leaves in range(20, 51, 5):\n    max_num += 1\n\n    # test_size = 0.2\n    # num_iters = 600\n    SEED = 86\n\n    print(f'test_size: {test_size} SEED={SEED}')\n\n    stratify_columns = ['target', 'cat3_grouped']\n    # stratify_columns = ['target', 'cat31']\n\n    # Split the train_df into training and testing sets\n    X_train, X_valid, y_train, y_valid = train_test_split(train, target,\n                                                          test_size=test_size,\n                                                          stratify=trn[stratify_columns],\n                                                          random_state=SEED)\n\n    pool_train = Pool(data=X_train, label=y_train, cat_features=cat_columns)\n    pool_valid = Pool(data=X_valid, label=y_valid, cat_features=cat_columns)\n    pool_test = Pool(data=test_df.drop(columns_no_to_model[1:], axis=1),\n                     cat_features=cat_columns)\n\n\n    num_folds = 4\n    skf = StratifiedKFold(n_splits=num_folds)\n    split_kf = KFold(n_splits=num_folds)\n\n    fit_on_full_train = False\n    use_grid_search = False\n    use_cv_folds = False\n    build_model = True\n    stratified = True\n    write_log = False\n\n    models, models_scores, predict_scores = [], [], []\n\n    loss_function = 'Logloss'\n\n    auto_class_weights = 'Balanced'\n#     auto_class_weights = None\n\n    eval_metric = 'Precision'\n#     eval_metric = 'TotalF1'\n\n    clf_params = dict(cat_features=cat_columns,\n                      auto_class_weights=auto_class_weights,\n                      loss_function=loss_function,\n                      eval_metric=eval_metric,\n#                       iterations=2000,  # попробовать столько итераций\n                      early_stopping_rounds=80,\n                      random_seed=SEED,\n                      task_type=\"GPU\",\n                      devices='0:1',\n                      gpu_ram_part=0.93,\n                      )\n\n    clf = CatBoostClassifier(**clf_params)\n\n    if use_grid_search:\n        # grid_params = {\n        #     'max_depth': [5, 6],\n        #     'learning_rate': [0.1, 0.15, 0.2],\n        # }\n        # grid_search_result = clf.grid_search(grid_params, train, target,\n        #                                      cv=skf,\n        #                                      stratified=True,\n        #                                      refit=True,\n        #                                      plot=True,\n        #                                      verbose=100,\n        #                                      )\n        # best_params = grid_search_result['params']\n        # models.append(clf)\n\n        study = optuna.create_study(\n            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"maximize\"\n        )\n        study.optimize(objective, n_trials=12, timeout=600)\n\n        print(\"Number of finished trials: {}\".format(len(study.trials)))\n        print(\"Best trial:\")\n        trial = study.best_trial\n\n        print(\"  Value: {}\".format(trial.value))\n        print(\"  Params: \")\n        for key, value in trial.params.items():\n            print(\"    {}: {}\".format(key, value))\n\n        best_params = trial.params\n\n        clf_params.update(best_params)\n        print('clf_params', clf_params)\n\n        clf = CatBoostClassifier(**clf_params)\n\n    if use_cv_folds:\n        if stratified:\n            skf_folds = skf.split(train, trn[['target', 'cat3_grouped']])\n        else:\n            skf_folds = split_kf.split(train)\n\n        for idx, (train_idx, valid_idx) in enumerate(skf_folds, 1):\n            print(f'Фолд {idx} из {num_folds}')\n            train_data = Pool(data=train.iloc[train_idx],\n                              label=target.iloc[train_idx],\n                              cat_features=cat_columns)\n            valid_data = Pool(data=train.iloc[valid_idx],\n                              label=target.iloc[valid_idx],\n                              cat_features=cat_columns)\n            model = clf\n            model.fit(train_data, eval_set=valid_data, use_best_model=True, verbose=100)\n            models.append(model)\n            if build_model:\n                DTS = (X_train, X_valid, y_train, y_valid, train, target, test_df, trn)\n                predict_scores = predict_test(idx, clf, DTS, max_num, submit_prefix='cb_')\n                models_scores.append(predict_scores)\n                acc_train, acc_valid, roc_auc, f1w, score_train, score_valid = predict_scores\n                comment = {'test_size': test_size,\n                           'SEED': SEED,\n                           'size': f'pool_{idx}'}\n                comment.update(data_cls.comment)\n                comment.update({'stratified': stratify_columns})\n                comment.update(clf.get_params())\n\n                with open(file_logs, mode='a') as log:\n                    # log.write('num;mdl;roc_auc;acc_train;acc_valid;sc_train;score;WF1;'\n                    #           'model_columns;exclude_columns;cat_columns;comment\\n')\n                    log.write(f'{max_num};cb;{roc_auc:.6f};{acc_train:.6f};{acc_valid:.6f};'\n                              f'{score_train:.6f};{score_valid:.6f};{f1w:.6f};'\n                              f'{train_df.columns.tolist()};'\n                              f'{data_cls.exclude_columns};{cat_columns};{comment}\\n')\n\n        best_params = {'iterations': [clf.tree_count_ for clf in models]}\n\n    else:\n        DTS = (X_train, X_valid, y_train, y_valid, train, target, test_df, trn)\n\n        clf.fit(pool_train, eval_set=pool_valid, use_best_model=True, verbose=50)\n\n        models.append(clf)\n\n        best_params = {'clf_iters': clf.tree_count_,\n                       'clf_lr': clf.get_all_params()['learning_rate']}\n\n        if build_model:\n            if not fit_on_full_train:\n                predict_scores = predict_test(0, clf, DTS, max_num, submit_prefix='cb_')\n\n            else:\n                predict_scores = predict_test('pool', clf, DTS, max_num, submit_prefix='cb_')\n                acc_train, acc_valid, roc_auc, f1w, score_train, score_valid = predict_scores\n                comment = {'test_size': test_size,\n                           'SEED': SEED,\n                           'size': 'pool'}\n                comment.update(data_cls.comment)\n                comment.update({'stratified': stratify_columns})\n                comment.update(models[0].get_params())\n\n                with open(file_logs, mode='a') as log:\n                    # log.write('num;mdl;roc_auc;acc_train;acc_valid;sc_train;score;WF1;'\n                    #           'model_columns;exclude_columns;cat_columns;comment\\n')\n                    log.write(f'{max_num};lg;{roc_auc:.6f};{acc_train:.6f};{acc_valid:.6f};'\n                              f'{score_train:.6f};{score_valid:.6f};{f1w:.6f};'\n                              f'{train_df.columns.tolist()};'\n                              f'{data_cls.exclude_columns};{cat_columns};{comment}\\n')\n\n                # обучение на всем трейне\n                print('Обучаюсь на всём трейне...')\n                clf_params['iterations'] = int(clf.tree_count_ * 1.1)\n                clf_params['learning_rate'] = clf.get_all_params()['learning_rate']\n                model = CatBoostClassifier(**clf_params)\n                model.fit(train, target, verbose=50, cat_features=cat_columns)\n                predict_scores = predict_test('full', model, DTS, max_num,\n                                              submit_prefix='cb_')\n\n        elif write_log:\n            predict_scores = predict_train_valid(0, clf, DTS, max_num)\n\n        best_params.update(clf.get_params())\n\n    print('best_params:', best_params)\n\n    if build_model or write_log:\n        if len(models) > 1:\n            predict_scores = [np.mean(arg) for arg in zip(*models_scores)]\n\n        acc_train, acc_valid, roc_auc, f1w, score_train, score_valid = predict_scores\n\n        print(f'Weighted F1-score = {f1w:.6f}')\n        print('Параметры модели:', clf.get_params())\n\n        print_time(start_time)\n\n        comment = {'test_size': test_size,\n                   'SEED': SEED,\n                   'clf_iters': clf.best_iteration_,\n                   'clf_lr': clf.get_params().get('learning_rate'),\n                   'stratified': stratified}\n        comment.update(data_cls.comment)\n        comment.update(clf.get_params())\n\n        with open(file_logs, mode='a') as log:\n            # log.write('num;mdl;roc_auc;acc_train;acc_valid;sc_train;score;WF1;'\n            #           'model_columns;exclude_columns;cat_columns;comment\\n')\n            log.write(f'{max_num};cb;{roc_auc:.6f};{acc_train:.6f};{acc_valid:.6f};'\n                      f'{score_train:.6f};{score_valid:.6f};{f1w:.6f};'\n                      f'{train_df.columns.tolist()};'\n                      f'{data_cls.exclude_columns};{cat_columns};{comment}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-05-30T15:19:43.298367Z","iopub.execute_input":"2023-05-30T15:19:43.298686Z","iopub.status.idle":"2023-05-30T15:52:56.427006Z","shell.execute_reply.started":"2023-05-30T15:19:43.298657Z","shell.execute_reply":"2023-05-30T15:52:56.425920Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Обучение Catboost классификатор...\nЧитаю датасеты с парами товаров\nВремя обработки: 40.3 сек\nПостобработка датасетов с парами товаров\nИсходный размер датасета в памяти равен 671.86 мб.\nКонечный размер датасета в памяти равен 447.35 мб.\nЭкономия памяти = 33.4%\nИсходный размер датасета в памяти равен 39.98 мб.\nКонечный размер датасета в памяти равен 26.73 мб.\nЭкономия памяти = 33.1%\nВ тренировочную выборку добавлено записей: 135013\nВремя обработки: 4 мин 26.3 сек\nОбучаюсь на колонках: ['target', 'atr_011', 'atr_012', 'atr_021', 'atr_022', 'atr_031', 'atr_032', 'atr_041', 'atr_042', 'atr_051', 'atr_052', 'atr_061', 'atr_062', 'atr_071', 'atr_072', 'atr_081', 'atr_082', 'atr_091', 'atr_092', 'atr_101', 'atr_102', 'atr_111', 'atr_112', 'atr_121', 'atr_122', 'atr_131', 'atr_132', 'atr_141', 'atr_142', 'atr_151', 'atr_152', 'atr_161', 'atr_162', 'atr_171', 'atr_172', 'atr_181', 'atr_182', 'atr_191', 'atr_192', 'atr_201', 'atr_202', 'atr_211', 'atr_212', 'atr_221', 'atr_222', 'atr_231', 'atr_232', 'atr_241', 'atr_242', 'atr_251', 'atr_252', 'atr_261', 'atr_262', 'atr_271', 'atr_272', 'atr_281', 'atr_282', 'atr_291', 'atr_292', 'atr_301', 'atr_302', 'atr_311', 'atr_312', 'atr_321', 'atr_322', 'atr_331', 'atr_332', 'atr_341', 'atr_342', 'atr_351', 'atr_352', 'atr_361', 'atr_362', 'atr_371', 'atr_372', 'atr_381', 'atr_382', 'atr_391', 'atr_392', 'atr_401', 'atr_402', 'atr_411', 'atr_412', 'atr_421', 'atr_422', 'atr_431', 'atr_432', 'atr_441', 'atr_442', 'atr_451', 'atr_452', 'atr_461', 'atr_462', 'atr_471', 'atr_472', 'atr_481', 'atr_482', 'atr_491', 'atr_492', 'atr_501', 'atr_502', 'atr_511', 'atr_512', 'atr_521', 'atr_522', 'atr_531', 'atr_532', 'atr_541', 'atr_542', 'atr_551', 'atr_552', 'atr_561', 'atr_562', 'atr_571', 'atr_572', 'atr_581', 'atr_582', 'atr_591', 'atr_592', 'atr_601', 'atr_602', 'atr_611', 'atr_612', 'atr_621', 'atr_622', 'atr_631', 'atr_632', 'atr_641', 'atr_642', 'atr_651', 'atr_652', 'atr_661', 'atr_662', 'atr_671', 'atr_672', 'atr_681', 'atr_682', 'atr_691', 'atr_692', 'atr_701', 'atr_702', 'atr_711', 'atr_712', 'atr_721', 'atr_722', 'atr_731', 'atr_732', 'atr_741', 'atr_742', 'atr_751', 'atr_752', 'atr_761', 'atr_762', 'atr_771', 'atr_772', 'atr_781', 'atr_782', 'atr_791', 'atr_792', 'atr_801', 'atr_802', 'atr_811', 'atr_812', 'atr_821', 'atr_822', 'atr_831', 'atr_832', 'atr_841', 'atr_842', 'atr_851', 'atr_852', 'atr_861', 'atr_862', 'atr_871', 'atr_872', 'atr_881', 'atr_882', 'atr_891', 'atr_892', 'atr_901', 'atr_902', 'atr_911', 'atr_912', 'atr_921', 'atr_922', 'atr_931', 'atr_932', 'atr_941', 'atr_942', 'atr_951', 'atr_952', 'atr_961', 'atr_962', 'atr_971', 'atr_972', 'atr_981', 'atr_982', 'atr_991', 'atr_992', 'cam_len1', 'cam_len2', 'cam_like', 'cam_st_len1', 'cam_st_len2', 'cat31', 'cat3_cnt1', 'cat3_cnt2', 'cat3_grouped', 'cat4_cnt1', 'cat4_cnt2', 'categories1', 'categories2', 'comp_atr_01', 'comp_atr_02', 'comp_atr_03', 'comp_atr_04', 'comp_atr_05', 'comp_atr_06', 'comp_atr_07', 'comp_atr_08', 'comp_atr_09', 'comp_atr_10', 'comp_atr_11', 'comp_atr_12', 'comp_atr_13', 'comp_atr_14', 'comp_atr_15', 'comp_atr_16', 'comp_atr_17', 'comp_atr_18', 'comp_atr_19', 'comp_atr_20', 'comp_atr_21', 'comp_atr_22', 'comp_atr_23', 'comp_atr_24', 'comp_atr_25', 'comp_atr_26', 'comp_atr_27', 'comp_atr_28', 'comp_atr_29', 'comp_atr_30', 'comp_atr_31', 'comp_atr_32', 'comp_atr_33', 'comp_atr_34', 'comp_atr_35', 'comp_atr_36', 'comp_atr_37', 'comp_atr_38', 'comp_atr_39', 'comp_atr_40', 'comp_atr_41', 'comp_atr_42', 'comp_atr_43', 'comp_atr_44', 'comp_atr_45', 'comp_atr_46', 'comp_atr_47', 'comp_atr_48', 'comp_atr_49', 'comp_atr_50', 'comp_atr_51', 'comp_atr_52', 'comp_atr_53', 'comp_atr_54', 'comp_atr_55', 'comp_atr_56', 'comp_atr_57', 'comp_atr_58', 'comp_atr_59', 'comp_atr_60', 'comp_atr_61', 'comp_atr_62', 'comp_atr_63', 'comp_atr_64', 'comp_atr_65', 'comp_atr_66', 'comp_atr_67', 'comp_atr_68', 'comp_atr_69', 'comp_atr_70', 'comp_atr_71', 'comp_atr_72', 'comp_atr_73', 'comp_atr_74', 'comp_atr_75', 'comp_atr_76', 'comp_atr_77', 'comp_atr_78', 'comp_atr_79', 'comp_atr_80', 'comp_atr_81', 'comp_atr_82', 'comp_atr_83', 'comp_atr_84', 'comp_atr_85', 'comp_atr_86', 'comp_atr_87', 'comp_atr_88', 'comp_atr_89', 'comp_atr_90', 'comp_atr_91', 'comp_atr_92', 'comp_atr_93', 'comp_atr_94', 'comp_atr_95', 'comp_atr_96', 'comp_atr_97', 'comp_atr_98', 'comp_atr_99', 'cos_main_pic', 'cos_name_bert', 'cos_name_pic', 'cos_pic_max', 'cos_pic_mean', 'cos_pic_min', 'eq_cats', 'fract_cam_len', 'fract_cam_st_len', 'fract_name_lm_len', 'fract_name_st_len', 'fuz_atr_01', 'fuz_atr_02', 'fuz_atr_03', 'fuz_atr_04', 'fuz_atr_05', 'fuz_atr_06', 'fuz_atr_07', 'fuz_atr_08', 'fuz_atr_09', 'fuz_atr_10', 'fuz_atr_11', 'fuz_atr_12', 'fuz_atr_13', 'fuz_atr_14', 'fuz_atr_15', 'fuz_atr_16', 'fuz_atr_17', 'fuz_atr_18', 'fuz_atr_19', 'fuz_atr_20', 'fuz_atr_21', 'fuz_atr_22', 'fuz_atr_23', 'fuz_atr_24', 'fuz_atr_25', 'fuz_atr_26', 'fuz_atr_27', 'fuz_atr_28', 'fuz_atr_29', 'fuz_atr_30', 'fuz_atr_31', 'fuz_atr_32', 'fuz_atr_33', 'fuz_atr_34', 'fuz_atr_35', 'fuz_atr_36', 'fuz_atr_37', 'fuz_atr_38', 'fuz_atr_39', 'fuz_atr_40', 'fuz_atr_41', 'fuz_atr_42', 'fuz_atr_43', 'fuz_atr_44', 'fuz_atr_45', 'fuz_atr_46', 'fuz_atr_47', 'fuz_atr_48', 'fuz_atr_49', 'fuz_atr_50', 'fuz_atr_51', 'fuz_atr_52', 'fuz_atr_53', 'fuz_atr_54', 'fuz_atr_55', 'fuz_atr_56', 'fuz_atr_57', 'fuz_atr_58', 'fuz_atr_59', 'fuz_atr_60', 'fuz_atr_61', 'fuz_atr_62', 'fuz_atr_63', 'fuz_atr_64', 'fuz_atr_65', 'fuz_atr_66', 'fuz_atr_67', 'fuz_atr_68', 'fuz_atr_69', 'fuz_atr_70', 'fuz_atr_71', 'fuz_atr_72', 'fuz_atr_73', 'fuz_atr_74', 'fuz_atr_75', 'fuz_atr_76', 'fuz_atr_77', 'fuz_atr_78', 'fuz_atr_79', 'fuz_atr_80', 'fuz_atr_81', 'fuz_atr_82', 'fuz_atr_83', 'fuz_atr_84', 'fuz_atr_85', 'fuz_atr_86', 'fuz_atr_87', 'fuz_atr_88', 'fuz_atr_89', 'fuz_atr_90', 'fuz_atr_91', 'fuz_atr_92', 'fuz_atr_93', 'fuz_atr_94', 'fuz_atr_95', 'fuz_atr_96', 'fuz_atr_97', 'fuz_atr_98', 'fuz_atr_99', 'fuz_cam_stem', 'fuz_name_lemm', 'fuz_name_stem', 'keys_1', 'keys_10', 'keys_11', 'keys_12', 'keys_13', 'keys_14', 'keys_15', 'keys_16', 'keys_17', 'keys_18', 'keys_19', 'keys_2', 'keys_20', 'keys_21', 'keys_22', 'keys_23', 'keys_24', 'keys_25', 'keys_26', 'keys_27', 'keys_28', 'keys_29', 'keys_3', 'keys_30', 'keys_31', 'keys_32', 'keys_33', 'keys_34', 'keys_35', 'keys_36', 'keys_37', 'keys_38', 'keys_39', 'keys_4', 'keys_40', 'keys_41', 'keys_42', 'keys_43', 'keys_44', 'keys_45', 'keys_46', 'keys_47', 'keys_48', 'keys_49', 'keys_5', 'keys_50', 'keys_51', 'keys_52', 'keys_53', 'keys_54', 'keys_55', 'keys_56', 'keys_57', 'keys_58', 'keys_59', 'keys_6', 'keys_60', 'keys_61', 'keys_62', 'keys_63', 'keys_64', 'keys_65', 'keys_66', 'keys_67', 'keys_68', 'keys_69', 'keys_7', 'keys_70', 'keys_71', 'keys_72', 'keys_73', 'keys_74', 'keys_75', 'keys_76', 'keys_77', 'keys_78', 'keys_79', 'keys_8', 'keys_80', 'keys_81', 'keys_82', 'keys_83', 'keys_84', 'keys_85', 'keys_86', 'keys_87', 'keys_88', 'keys_89', 'keys_9', 'keys_90', 'keys_91', 'keys_92', 'keys_93', 'keys_94', 'keys_95', 'keys_96', 'keys_97', 'keys_98', 'keys_99', 'mp1_0', 'mp1_1', 'mp1_10', 'mp1_100', 'mp1_101', 'mp1_102', 'mp1_103', 'mp1_104', 'mp1_105', 'mp1_106', 'mp1_107', 'mp1_108', 'mp1_109', 'mp1_11', 'mp1_110', 'mp1_111', 'mp1_112', 'mp1_113', 'mp1_114', 'mp1_115', 'mp1_116', 'mp1_117', 'mp1_118', 'mp1_119', 'mp1_12', 'mp1_120', 'mp1_121', 'mp1_122', 'mp1_123', 'mp1_124', 'mp1_125', 'mp1_126', 'mp1_127', 'mp1_13', 'mp1_14', 'mp1_15', 'mp1_16', 'mp1_17', 'mp1_18', 'mp1_19', 'mp1_2', 'mp1_20', 'mp1_21', 'mp1_22', 'mp1_23', 'mp1_24', 'mp1_25', 'mp1_26', 'mp1_27', 'mp1_28', 'mp1_29', 'mp1_3', 'mp1_30', 'mp1_31', 'mp1_32', 'mp1_33', 'mp1_34', 'mp1_35', 'mp1_36', 'mp1_37', 'mp1_38', 'mp1_39', 'mp1_4', 'mp1_40', 'mp1_41', 'mp1_42', 'mp1_43', 'mp1_44', 'mp1_45', 'mp1_46', 'mp1_47', 'mp1_48', 'mp1_49', 'mp1_5', 'mp1_50', 'mp1_51', 'mp1_52', 'mp1_53', 'mp1_54', 'mp1_55', 'mp1_56', 'mp1_57', 'mp1_58', 'mp1_59', 'mp1_6', 'mp1_60', 'mp1_61', 'mp1_62', 'mp1_63', 'mp1_64', 'mp1_65', 'mp1_66', 'mp1_67', 'mp1_68', 'mp1_69', 'mp1_7', 'mp1_70', 'mp1_71', 'mp1_72', 'mp1_73', 'mp1_74', 'mp1_75', 'mp1_76', 'mp1_77', 'mp1_78', 'mp1_79', 'mp1_8', 'mp1_80', 'mp1_81', 'mp1_82', 'mp1_83', 'mp1_84', 'mp1_85', 'mp1_86', 'mp1_87', 'mp1_88', 'mp1_89', 'mp1_9', 'mp1_90', 'mp1_91', 'mp1_92', 'mp1_93', 'mp1_94', 'mp1_95', 'mp1_96', 'mp1_97', 'mp1_98', 'mp1_99', 'mp2_0', 'mp2_1', 'mp2_10', 'mp2_100', 'mp2_101', 'mp2_102', 'mp2_103', 'mp2_104', 'mp2_105', 'mp2_106', 'mp2_107', 'mp2_108', 'mp2_109', 'mp2_11', 'mp2_110', 'mp2_111', 'mp2_112', 'mp2_113', 'mp2_114', 'mp2_115', 'mp2_116', 'mp2_117', 'mp2_118', 'mp2_119', 'mp2_12', 'mp2_120', 'mp2_121', 'mp2_122', 'mp2_123', 'mp2_124', 'mp2_125', 'mp2_126', 'mp2_127', 'mp2_13', 'mp2_14', 'mp2_15', 'mp2_16', 'mp2_17', 'mp2_18', 'mp2_19', 'mp2_2', 'mp2_20', 'mp2_21', 'mp2_22', 'mp2_23', 'mp2_24', 'mp2_25', 'mp2_26', 'mp2_27', 'mp2_28', 'mp2_29', 'mp2_3', 'mp2_30', 'mp2_31', 'mp2_32', 'mp2_33', 'mp2_34', 'mp2_35', 'mp2_36', 'mp2_37', 'mp2_38', 'mp2_39', 'mp2_4', 'mp2_40', 'mp2_41', 'mp2_42', 'mp2_43', 'mp2_44', 'mp2_45', 'mp2_46', 'mp2_47', 'mp2_48', 'mp2_49', 'mp2_5', 'mp2_50', 'mp2_51', 'mp2_52', 'mp2_53', 'mp2_54', 'mp2_55', 'mp2_56', 'mp2_57', 'mp2_58', 'mp2_59', 'mp2_6', 'mp2_60', 'mp2_61', 'mp2_62', 'mp2_63', 'mp2_64', 'mp2_65', 'mp2_66', 'mp2_67', 'mp2_68', 'mp2_69', 'mp2_7', 'mp2_70', 'mp2_71', 'mp2_72', 'mp2_73', 'mp2_74', 'mp2_75', 'mp2_76', 'mp2_77', 'mp2_78', 'mp2_79', 'mp2_8', 'mp2_80', 'mp2_81', 'mp2_82', 'mp2_83', 'mp2_84', 'mp2_85', 'mp2_86', 'mp2_87', 'mp2_88', 'mp2_89', 'mp2_9', 'mp2_90', 'mp2_91', 'mp2_92', 'mp2_93', 'mp2_94', 'mp2_95', 'mp2_96', 'mp2_97', 'mp2_98', 'mp2_99', 'name_lm_len1', 'name_lm_len2', 'name_st_len1', 'name_st_len2', 'nb1_0', 'nb1_1', 'nb1_10', 'nb1_11', 'nb1_12', 'nb1_13', 'nb1_14', 'nb1_15', 'nb1_16', 'nb1_17', 'nb1_18', 'nb1_19', 'nb1_2', 'nb1_20', 'nb1_21', 'nb1_22', 'nb1_23', 'nb1_24', 'nb1_25', 'nb1_26', 'nb1_27', 'nb1_28', 'nb1_29', 'nb1_3', 'nb1_30', 'nb1_31', 'nb1_32', 'nb1_33', 'nb1_34', 'nb1_35', 'nb1_36', 'nb1_37', 'nb1_38', 'nb1_39', 'nb1_4', 'nb1_40', 'nb1_41', 'nb1_42', 'nb1_43', 'nb1_44', 'nb1_45', 'nb1_46', 'nb1_47', 'nb1_48', 'nb1_49', 'nb1_5', 'nb1_50', 'nb1_51', 'nb1_52', 'nb1_53', 'nb1_54', 'nb1_55', 'nb1_56', 'nb1_57', 'nb1_58', 'nb1_59', 'nb1_6', 'nb1_60', 'nb1_61', 'nb1_62', 'nb1_63', 'nb1_7', 'nb1_8', 'nb1_9', 'nb2_0', 'nb2_1', 'nb2_10', 'nb2_11', 'nb2_12', 'nb2_13', 'nb2_14', 'nb2_15', 'nb2_16', 'nb2_17', 'nb2_18', 'nb2_19', 'nb2_2', 'nb2_20', 'nb2_21', 'nb2_22', 'nb2_23', 'nb2_24', 'nb2_25', 'nb2_26', 'nb2_27', 'nb2_28', 'nb2_29', 'nb2_3', 'nb2_30', 'nb2_31', 'nb2_32', 'nb2_33', 'nb2_34', 'nb2_35', 'nb2_36', 'nb2_37', 'nb2_38', 'nb2_39', 'nb2_4', 'nb2_40', 'nb2_41', 'nb2_42', 'nb2_43', 'nb2_44', 'nb2_45', 'nb2_46', 'nb2_47', 'nb2_48', 'nb2_49', 'nb2_5', 'nb2_50', 'nb2_51', 'nb2_52', 'nb2_53', 'nb2_54', 'nb2_55', 'nb2_56', 'nb2_57', 'nb2_58', 'nb2_59', 'nb2_6', 'nb2_60', 'nb2_61', 'nb2_62', 'nb2_63', 'nb2_7', 'nb2_8', 'nb2_9', 'variantid1', 'variantid2']\nКатегорийные колонки: ['cat31', 'categories1', 'categories2', 'cat3_grouped', 'atr_011', 'atr_012', 'atr_021', 'atr_022', 'atr_031', 'atr_032', 'atr_041', 'atr_042', 'atr_051', 'atr_052', 'atr_061', 'atr_062', 'atr_071', 'atr_072', 'atr_081', 'atr_082', 'atr_091', 'atr_092', 'atr_101', 'atr_102', 'atr_111', 'atr_112', 'atr_121', 'atr_122', 'atr_131', 'atr_132', 'atr_141', 'atr_142', 'atr_151', 'atr_152', 'atr_161', 'atr_162', 'atr_171', 'atr_172', 'atr_181', 'atr_182', 'atr_191', 'atr_192', 'atr_201', 'atr_202', 'atr_211', 'atr_212', 'atr_221', 'atr_222', 'atr_231', 'atr_232', 'atr_241', 'atr_242', 'atr_251', 'atr_252', 'atr_261', 'atr_262', 'atr_271', 'atr_272', 'atr_281', 'atr_282', 'atr_291', 'atr_292', 'atr_301', 'atr_302', 'atr_311', 'atr_312', 'atr_321', 'atr_322', 'atr_331', 'atr_332', 'atr_341', 'atr_342', 'atr_351', 'atr_352', 'atr_361', 'atr_362', 'atr_371', 'atr_372', 'atr_381', 'atr_382', 'atr_391', 'atr_392', 'atr_401', 'atr_402', 'atr_411', 'atr_412', 'atr_421', 'atr_422', 'atr_431', 'atr_432', 'atr_441', 'atr_442', 'atr_451', 'atr_452', 'atr_461', 'atr_462', 'atr_471', 'atr_472', 'atr_481', 'atr_482', 'atr_491', 'atr_492', 'atr_501', 'atr_502', 'atr_511', 'atr_512', 'atr_521', 'atr_522', 'atr_531', 'atr_532', 'atr_541', 'atr_542', 'atr_551', 'atr_552', 'atr_561', 'atr_562', 'atr_571', 'atr_572', 'atr_581', 'atr_582', 'atr_591', 'atr_592', 'atr_601', 'atr_602', 'atr_611', 'atr_612', 'atr_621', 'atr_622', 'atr_631', 'atr_632', 'atr_641', 'atr_642', 'atr_651', 'atr_652', 'atr_661', 'atr_662', 'atr_671', 'atr_672', 'atr_681', 'atr_682', 'atr_691', 'atr_692', 'atr_701', 'atr_702', 'atr_711', 'atr_712', 'atr_721', 'atr_722', 'atr_731', 'atr_732', 'atr_741', 'atr_742', 'atr_751', 'atr_752', 'atr_761', 'atr_762', 'atr_771', 'atr_772', 'atr_781', 'atr_782', 'atr_791', 'atr_792', 'atr_801', 'atr_802', 'atr_811', 'atr_812', 'atr_821', 'atr_822', 'atr_831', 'atr_832', 'atr_841', 'atr_842', 'atr_851', 'atr_852', 'atr_861', 'atr_862', 'atr_871', 'atr_872', 'atr_881', 'atr_882', 'atr_891', 'atr_892', 'atr_901', 'atr_902', 'atr_911', 'atr_912', 'atr_921', 'atr_922', 'atr_931', 'atr_932', 'atr_941', 'atr_942', 'atr_951', 'atr_952', 'atr_961', 'atr_962', 'atr_971', 'atr_972', 'atr_981', 'atr_982', 'atr_991', 'atr_992', 'keys_1', 'keys_10', 'keys_11', 'keys_12', 'keys_13', 'keys_14', 'keys_15', 'keys_16', 'keys_17', 'keys_18', 'keys_19', 'keys_2', 'keys_20', 'keys_21', 'keys_22', 'keys_23', 'keys_24', 'keys_25', 'keys_26', 'keys_27', 'keys_28', 'keys_29', 'keys_3', 'keys_30', 'keys_31', 'keys_32', 'keys_33', 'keys_34', 'keys_35', 'keys_36', 'keys_37', 'keys_38', 'keys_39', 'keys_4', 'keys_40', 'keys_41', 'keys_42', 'keys_43', 'keys_44', 'keys_45', 'keys_46', 'keys_47', 'keys_48', 'keys_49', 'keys_5', 'keys_50', 'keys_51', 'keys_52', 'keys_53', 'keys_54', 'keys_55', 'keys_56', 'keys_57', 'keys_58', 'keys_59', 'keys_6', 'keys_60', 'keys_61', 'keys_62', 'keys_63', 'keys_64', 'keys_65', 'keys_66', 'keys_67', 'keys_68', 'keys_69', 'keys_7', 'keys_70', 'keys_71', 'keys_72', 'keys_73', 'keys_74', 'keys_75', 'keys_76', 'keys_77', 'keys_78', 'keys_79', 'keys_8', 'keys_80', 'keys_81', 'keys_82', 'keys_83', 'keys_84', 'keys_85', 'keys_86', 'keys_87', 'keys_88', 'keys_89', 'keys_9', 'keys_90', 'keys_91', 'keys_92', 'keys_93', 'keys_94', 'keys_95', 'keys_96', 'keys_97', 'keys_98', 'keys_99']\nИсключенные колонки: ['name', 'color', 'pic_emb', 'ch_att_map', 'cam', 'cat', 'name_pic', 'cos_pics', 'cos_values', 'cos_idxs', 'name_norm1', 'name_stem1', 'name_lemm1', 'name_norm2', 'name_stem2', 'name_lemm2', 'cam_norm1', 'cam_stem1', 'cam_lemm1', 'cam_norm2', 'cam_stem2', 'cam_lemm2', 'name_stem_lemm1', 'name_stem_lemm2', 'pic_emb_mean', 'cam_lemm', 'name_stem_lemm', 'cos_pics', 'epic', 'fract_name_stem_lemm', 'frset_cam_stem', 'frset_name_lemm', 'frset_name_stem', 'frset_name_stem_lemm', 'epic1', 'epic2', 'epic1', 'epic2', 'epic1', 'epic2', 'cat32']\ncat31 = Измерительный инструмент\ncat31 = Электромобили детские\ncat31 = Конструкторы\ncat31 = Товары для праздников\ncat31 = Фитнес-браслеты и кардиомониторы\ncat31 = Автомобильные масла\ncat31 = Автозвук\ncat31 = Детские рюкзаки, ранцы, сумки\ncat31 = Калибратор\ncat31 = Бинокли и подзорные трубы\ncat31 = Часы\ncat31 = Моющие и чистящие средства\ncat31 = Бумага\ncat31 = Фонари и аксессуары\ncat31 = Спортивные чехлы и сумки\ncat31 = Бумажная продукция\ncat31 = Автоаксессуары и принадлежности\ncat31 = Солнечная, ветряная электростанция\ncat31 = Аксессуар для пылесоса\ncat31 = Ниппели, вентили и секретки\ncat31 = Декор и интерьер\ncat31 = Обучающие игры\ncat31 = Электрооборудование автомобиля\ncat31 = Печати и штампы\ncat31 = Аккумуляторы и аксессуары\ncat31 = Секс игрушки\ncat31 = Радиоуправляемые игрушки\ncat31 = Набор для рукоделия, творчества\ncat31 = Сумка\ncat31 = Ручки, замки и фурнитура\ncat31 = Погодные станции и датчики\ncat31 = Товары для велоспорта\ncat31 = Автомагнитола\ncat31 = Мотоаксессуары\ncat31 = Электроустановочные изделия\ncat31 = Шторы и карнизы\ncat31 = Паяльное оборудование\ncat31 = Светильник\ncat31 = Хранение продуктов\ncat31 = Сувениры и подарки\ncat31 = Охота и стрельба\ncat31 = Аксессуар для рукоделия\ncat31 = Запчасти, аксессуары\ncat31 = Утюги и отпариватели\nРазмер train_df = (442014, 913), test_df = (18084, 912)\ntest_size: 0.13 SEED=86\nLearning rate set to 0.042934\n0:\tlearn: 0.7535694\ttest: 0.7555272\tbest: 0.7555272 (0)\ttotal: 781ms\tremaining: 13m\n50:\tlearn: 0.7985727\ttest: 0.8010001\tbest: 0.8010001 (50)\ttotal: 30.3s\tremaining: 9m 23s\n100:\tlearn: 0.8132603\ttest: 0.8159959\tbest: 0.8161077 (99)\ttotal: 59.3s\tremaining: 8m 47s\n150:\tlearn: 0.8231224\ttest: 0.8246381\tbest: 0.8246381 (150)\ttotal: 1m 28s\tremaining: 8m 15s\n200:\tlearn: 0.8298571\ttest: 0.8315930\tbest: 0.8315930 (200)\ttotal: 1m 56s\tremaining: 7m 44s\n250:\tlearn: 0.8377709\ttest: 0.8387602\tbest: 0.8390069 (245)\ttotal: 2m 26s\tremaining: 7m 17s\n300:\tlearn: 0.8430252\ttest: 0.8448939\tbest: 0.8448939 (300)\ttotal: 2m 56s\tremaining: 6m 50s\n350:\tlearn: 0.8471252\ttest: 0.8488989\tbest: 0.8491178 (348)\ttotal: 3m 27s\tremaining: 6m 23s\n400:\tlearn: 0.8497159\ttest: 0.8519045\tbest: 0.8519045 (400)\ttotal: 3m 57s\tremaining: 5m 55s\n450:\tlearn: 0.8520670\ttest: 0.8544231\tbest: 0.8545383 (446)\ttotal: 4m 27s\tremaining: 5m 25s\n500:\tlearn: 0.8544855\ttest: 0.8568884\tbest: 0.8569270 (499)\ttotal: 4m 58s\tremaining: 4m 57s\n550:\tlearn: 0.8572149\ttest: 0.8594758\tbest: 0.8594758 (550)\ttotal: 5m 29s\tremaining: 4m 28s\n600:\tlearn: 0.8597365\ttest: 0.8616904\tbest: 0.8617453 (599)\ttotal: 6m\tremaining: 3m 59s\n650:\tlearn: 0.8611996\ttest: 0.8629137\tbest: 0.8630547 (646)\ttotal: 6m 30s\tremaining: 3m 29s\n700:\tlearn: 0.8624612\ttest: 0.8638225\tbest: 0.8638225 (700)\ttotal: 7m 1s\tremaining: 2m 59s\n750:\tlearn: 0.8638504\ttest: 0.8651422\tbest: 0.8651422 (750)\ttotal: 7m 31s\tremaining: 2m 29s\n800:\tlearn: 0.8654606\ttest: 0.8663470\tbest: 0.8664452 (798)\ttotal: 8m 1s\tremaining: 1m 59s\n850:\tlearn: 0.8664710\ttest: 0.8670882\tbest: 0.8671595 (849)\ttotal: 8m 33s\tremaining: 1m 29s\n900:\tlearn: 0.8675006\ttest: 0.8679618\tbest: 0.8679618 (900)\ttotal: 9m 3s\tremaining: 59.8s\n950:\tlearn: 0.8681219\ttest: 0.8683996\tbest: 0.8684186 (948)\ttotal: 9m 33s\tremaining: 29.6s\n999:\tlearn: 0.8689246\ttest: 0.8690715\tbest: 0.8690715 (999)\ttotal: 10m 4s\tremaining: 0us\nbestTest = 0.8690715064\nbestIteration = 999\npredict_proba.shape: (18084,) classes: [0, 1]\nScore auc_macro = train:0.899891 valid:0.875386\nAccuracy train:0.8769737252699245 valid:0.8608993769795691 roc_auc:0.9349764607460485 F1:0.882585\nbest_params: {'clf_iters': 1000, 'clf_lr': 0.04293400049209595, 'loss_function': 'Logloss', 'random_seed': 86, 'auto_class_weights': 'Balanced', 'eval_metric': 'Precision', 'gpu_ram_part': 0.93, 'task_type': 'GPU', 'devices': '0:1', 'early_stopping_rounds': 80, 'cat_features': ['cat31', 'categories1', 'categories2', 'cat3_grouped', 'atr_011', 'atr_012', 'atr_021', 'atr_022', 'atr_031', 'atr_032', 'atr_041', 'atr_042', 'atr_051', 'atr_052', 'atr_061', 'atr_062', 'atr_071', 'atr_072', 'atr_081', 'atr_082', 'atr_091', 'atr_092', 'atr_101', 'atr_102', 'atr_111', 'atr_112', 'atr_121', 'atr_122', 'atr_131', 'atr_132', 'atr_141', 'atr_142', 'atr_151', 'atr_152', 'atr_161', 'atr_162', 'atr_171', 'atr_172', 'atr_181', 'atr_182', 'atr_191', 'atr_192', 'atr_201', 'atr_202', 'atr_211', 'atr_212', 'atr_221', 'atr_222', 'atr_231', 'atr_232', 'atr_241', 'atr_242', 'atr_251', 'atr_252', 'atr_261', 'atr_262', 'atr_271', 'atr_272', 'atr_281', 'atr_282', 'atr_291', 'atr_292', 'atr_301', 'atr_302', 'atr_311', 'atr_312', 'atr_321', 'atr_322', 'atr_331', 'atr_332', 'atr_341', 'atr_342', 'atr_351', 'atr_352', 'atr_361', 'atr_362', 'atr_371', 'atr_372', 'atr_381', 'atr_382', 'atr_391', 'atr_392', 'atr_401', 'atr_402', 'atr_411', 'atr_412', 'atr_421', 'atr_422', 'atr_431', 'atr_432', 'atr_441', 'atr_442', 'atr_451', 'atr_452', 'atr_461', 'atr_462', 'atr_471', 'atr_472', 'atr_481', 'atr_482', 'atr_491', 'atr_492', 'atr_501', 'atr_502', 'atr_511', 'atr_512', 'atr_521', 'atr_522', 'atr_531', 'atr_532', 'atr_541', 'atr_542', 'atr_551', 'atr_552', 'atr_561', 'atr_562', 'atr_571', 'atr_572', 'atr_581', 'atr_582', 'atr_591', 'atr_592', 'atr_601', 'atr_602', 'atr_611', 'atr_612', 'atr_621', 'atr_622', 'atr_631', 'atr_632', 'atr_641', 'atr_642', 'atr_651', 'atr_652', 'atr_661', 'atr_662', 'atr_671', 'atr_672', 'atr_681', 'atr_682', 'atr_691', 'atr_692', 'atr_701', 'atr_702', 'atr_711', 'atr_712', 'atr_721', 'atr_722', 'atr_731', 'atr_732', 'atr_741', 'atr_742', 'atr_751', 'atr_752', 'atr_761', 'atr_762', 'atr_771', 'atr_772', 'atr_781', 'atr_782', 'atr_791', 'atr_792', 'atr_801', 'atr_802', 'atr_811', 'atr_812', 'atr_821', 'atr_822', 'atr_831', 'atr_832', 'atr_841', 'atr_842', 'atr_851', 'atr_852', 'atr_861', 'atr_862', 'atr_871', 'atr_872', 'atr_881', 'atr_882', 'atr_891', 'atr_892', 'atr_901', 'atr_902', 'atr_911', 'atr_912', 'atr_921', 'atr_922', 'atr_931', 'atr_932', 'atr_941', 'atr_942', 'atr_951', 'atr_952', 'atr_961', 'atr_962', 'atr_971', 'atr_972', 'atr_981', 'atr_982', 'atr_991', 'atr_992', 'keys_1', 'keys_10', 'keys_11', 'keys_12', 'keys_13', 'keys_14', 'keys_15', 'keys_16', 'keys_17', 'keys_18', 'keys_19', 'keys_2', 'keys_20', 'keys_21', 'keys_22', 'keys_23', 'keys_24', 'keys_25', 'keys_26', 'keys_27', 'keys_28', 'keys_29', 'keys_3', 'keys_30', 'keys_31', 'keys_32', 'keys_33', 'keys_34', 'keys_35', 'keys_36', 'keys_37', 'keys_38', 'keys_39', 'keys_4', 'keys_40', 'keys_41', 'keys_42', 'keys_43', 'keys_44', 'keys_45', 'keys_46', 'keys_47', 'keys_48', 'keys_49', 'keys_5', 'keys_50', 'keys_51', 'keys_52', 'keys_53', 'keys_54', 'keys_55', 'keys_56', 'keys_57', 'keys_58', 'keys_59', 'keys_6', 'keys_60', 'keys_61', 'keys_62', 'keys_63', 'keys_64', 'keys_65', 'keys_66', 'keys_67', 'keys_68', 'keys_69', 'keys_7', 'keys_70', 'keys_71', 'keys_72', 'keys_73', 'keys_74', 'keys_75', 'keys_76', 'keys_77', 'keys_78', 'keys_79', 'keys_8', 'keys_80', 'keys_81', 'keys_82', 'keys_83', 'keys_84', 'keys_85', 'keys_86', 'keys_87', 'keys_88', 'keys_89', 'keys_9', 'keys_90', 'keys_91', 'keys_92', 'keys_93', 'keys_94', 'keys_95', 'keys_96', 'keys_97', 'keys_98', 'keys_99']}\nWeighted F1-score = 0.882585\nПараметры модели: {'loss_function': 'Logloss', 'random_seed': 86, 'auto_class_weights': 'Balanced', 'eval_metric': 'Precision', 'gpu_ram_part': 0.93, 'task_type': 'GPU', 'devices': '0:1', 'early_stopping_rounds': 80, 'cat_features': ['cat31', 'categories1', 'categories2', 'cat3_grouped', 'atr_011', 'atr_012', 'atr_021', 'atr_022', 'atr_031', 'atr_032', 'atr_041', 'atr_042', 'atr_051', 'atr_052', 'atr_061', 'atr_062', 'atr_071', 'atr_072', 'atr_081', 'atr_082', 'atr_091', 'atr_092', 'atr_101', 'atr_102', 'atr_111', 'atr_112', 'atr_121', 'atr_122', 'atr_131', 'atr_132', 'atr_141', 'atr_142', 'atr_151', 'atr_152', 'atr_161', 'atr_162', 'atr_171', 'atr_172', 'atr_181', 'atr_182', 'atr_191', 'atr_192', 'atr_201', 'atr_202', 'atr_211', 'atr_212', 'atr_221', 'atr_222', 'atr_231', 'atr_232', 'atr_241', 'atr_242', 'atr_251', 'atr_252', 'atr_261', 'atr_262', 'atr_271', 'atr_272', 'atr_281', 'atr_282', 'atr_291', 'atr_292', 'atr_301', 'atr_302', 'atr_311', 'atr_312', 'atr_321', 'atr_322', 'atr_331', 'atr_332', 'atr_341', 'atr_342', 'atr_351', 'atr_352', 'atr_361', 'atr_362', 'atr_371', 'atr_372', 'atr_381', 'atr_382', 'atr_391', 'atr_392', 'atr_401', 'atr_402', 'atr_411', 'atr_412', 'atr_421', 'atr_422', 'atr_431', 'atr_432', 'atr_441', 'atr_442', 'atr_451', 'atr_452', 'atr_461', 'atr_462', 'atr_471', 'atr_472', 'atr_481', 'atr_482', 'atr_491', 'atr_492', 'atr_501', 'atr_502', 'atr_511', 'atr_512', 'atr_521', 'atr_522', 'atr_531', 'atr_532', 'atr_541', 'atr_542', 'atr_551', 'atr_552', 'atr_561', 'atr_562', 'atr_571', 'atr_572', 'atr_581', 'atr_582', 'atr_591', 'atr_592', 'atr_601', 'atr_602', 'atr_611', 'atr_612', 'atr_621', 'atr_622', 'atr_631', 'atr_632', 'atr_641', 'atr_642', 'atr_651', 'atr_652', 'atr_661', 'atr_662', 'atr_671', 'atr_672', 'atr_681', 'atr_682', 'atr_691', 'atr_692', 'atr_701', 'atr_702', 'atr_711', 'atr_712', 'atr_721', 'atr_722', 'atr_731', 'atr_732', 'atr_741', 'atr_742', 'atr_751', 'atr_752', 'atr_761', 'atr_762', 'atr_771', 'atr_772', 'atr_781', 'atr_782', 'atr_791', 'atr_792', 'atr_801', 'atr_802', 'atr_811', 'atr_812', 'atr_821', 'atr_822', 'atr_831', 'atr_832', 'atr_841', 'atr_842', 'atr_851', 'atr_852', 'atr_861', 'atr_862', 'atr_871', 'atr_872', 'atr_881', 'atr_882', 'atr_891', 'atr_892', 'atr_901', 'atr_902', 'atr_911', 'atr_912', 'atr_921', 'atr_922', 'atr_931', 'atr_932', 'atr_941', 'atr_942', 'atr_951', 'atr_952', 'atr_961', 'atr_962', 'atr_971', 'atr_972', 'atr_981', 'atr_982', 'atr_991', 'atr_992', 'keys_1', 'keys_10', 'keys_11', 'keys_12', 'keys_13', 'keys_14', 'keys_15', 'keys_16', 'keys_17', 'keys_18', 'keys_19', 'keys_2', 'keys_20', 'keys_21', 'keys_22', 'keys_23', 'keys_24', 'keys_25', 'keys_26', 'keys_27', 'keys_28', 'keys_29', 'keys_3', 'keys_30', 'keys_31', 'keys_32', 'keys_33', 'keys_34', 'keys_35', 'keys_36', 'keys_37', 'keys_38', 'keys_39', 'keys_4', 'keys_40', 'keys_41', 'keys_42', 'keys_43', 'keys_44', 'keys_45', 'keys_46', 'keys_47', 'keys_48', 'keys_49', 'keys_5', 'keys_50', 'keys_51', 'keys_52', 'keys_53', 'keys_54', 'keys_55', 'keys_56', 'keys_57', 'keys_58', 'keys_59', 'keys_6', 'keys_60', 'keys_61', 'keys_62', 'keys_63', 'keys_64', 'keys_65', 'keys_66', 'keys_67', 'keys_68', 'keys_69', 'keys_7', 'keys_70', 'keys_71', 'keys_72', 'keys_73', 'keys_74', 'keys_75', 'keys_76', 'keys_77', 'keys_78', 'keys_79', 'keys_8', 'keys_80', 'keys_81', 'keys_82', 'keys_83', 'keys_84', 'keys_85', 'keys_86', 'keys_87', 'keys_88', 'keys_89', 'keys_9', 'keys_90', 'keys_91', 'keys_92', 'keys_93', 'keys_94', 'keys_95', 'keys_96', 'keys_97', 'keys_98', 'keys_99']}\nВремя обработки: 32 мин 32.5 сек\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x700 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import display, FileLink\nfrom zipfile import ZipFile, ZIP_DEFLATED as ZD\nfrom glob import glob\n\nfiles = glob(f'{PREDICTIONS_DIR}/*.csv') + glob(f'{PREDICTIONS_DIR}/*.logs')\nzip_filename = WORK_PATH.joinpath('predictions.zip')\nwith ZipFile(zip_filename, 'w',  compression=ZD, compresslevel=9) as zip_file:\n    for filename in files:\n        print(filename)\n        zip_file.write(filename)\nFileLink(zip_filename)","metadata":{"execution":{"iopub.status.busy":"2023-05-30T16:07:10.046066Z","iopub.execute_input":"2023-05-30T16:07:10.046470Z","iopub.status.idle":"2023-05-30T16:07:15.334958Z","shell.execute_reply.started":"2023-05-30T16:07:10.046441Z","shell.execute_reply":"2023-05-30T16:07:15.333952Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"./cb_submit_086.csv\n./cb_train_086.csv\n./scores.logs\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/predictions.zip","text/html":"<a href='predictions.zip' target='_blank'>predictions.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-05-30T15:53:01.429954Z","iopub.execute_input":"2023-05-30T15:53:01.430300Z","iopub.status.idle":"2023-05-30T15:53:01.769003Z","shell.execute_reply.started":"2023-05-30T15:53:01.430276Z","shell.execute_reply":"2023-05-30T15:53:01.767768Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"3768"},"metadata":{}}]}]}